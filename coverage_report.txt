/home/martin/git/fugrip/src/collector_phases.rs:
    1|       |use crate::memory::*;
    2|       |use crate::types::*;
    3|       |use std::sync::atomic::{AtomicBool, AtomicUsize, Ordering};
    4|       |use std::sync::{Condvar, Mutex};
    5|       |
    6|       |/// Registration data for threads participating in cooperative GC.
    7|       |///
    8|       |/// This structure tracks thread information needed for precise garbage collection,
    9|       |/// including stack bounds and root sets. Each participating thread registers itself
   10|       |/// with the collector to enable cooperative root scanning.
   11|       |///
   12|       |/// # Examples
   13|       |///
   14|       |/// ```
   15|       |/// use fugrip::collector_phases::ThreadRegistration;
   16|       |/// use std::sync::atomic::{AtomicBool, AtomicUsize};
   17|       |/// use std::thread;
   18|       |/// 
   19|       |/// // Create a thread registration (normally done automatically)
   20|       |/// let registration = ThreadRegistration {
   21|       |///     thread_id: thread::current().id(),
   22|       |///     stack_base: 0x1000000,  // Example stack base
   23|       |///     stack_bounds: (0x1000000, 0x1100000), // 1MB stack
   24|       |///     last_known_sp: AtomicUsize::new(0x1050000),
   25|       |///     local_roots: Vec::new(),
   26|       |///     is_active: AtomicBool::new(true),
   27|       |/// };
   28|       |/// 
   29|       |/// // Check if thread is active
   30|       |/// assert!(registration.is_active.load(std::sync::atomic::Ordering::Acquire));
   31|       |/// ```
   32|       |#[derive(Debug)]
   33|       |pub struct ThreadRegistration {
   34|       |    pub thread_id: std::thread::ThreadId,
   35|       |    pub stack_base: usize, // Using usize instead of raw pointers for thread safety
   36|       |    pub stack_bounds: (usize, usize), // (bottom, top) as addresses
   37|       |    pub last_known_sp: AtomicUsize, // Using AtomicUsize instead of AtomicPtr
   38|       |    pub local_roots: Vec<SendPtr<GcHeader<()>>>,
   39|       |    pub is_active: AtomicBool,
   40|       |}
   41|       |
   42|       |pub struct CollectorState {
   43|       |    pub phase: AtomicUsize, // CollectorPhase as usize
   44|       |    pub marking_active: AtomicBool,
   45|       |    pub allocation_color: AtomicBool, // true = black, false = white
   46|       |
   47|       |    // Parallel marking infrastructure
   48|       |    pub global_mark_stack: Mutex<Vec<SendPtr<GcHeader<()>>>>,
   49|       |    pub worker_count: AtomicUsize,
   50|       |    pub workers_finished: AtomicUsize,
   51|       |
   52|       |    // Handshake mechanism
   53|       |    pub handshake_requested: AtomicBool,
   54|       |    pub handshake_completed: Condvar,
   55|       |    pub active_mutator_count: AtomicUsize,
   56|       |    pub handshake_acknowledgments: AtomicUsize,
   57|       |
   58|       |    // Suspension for fork() safety
   59|       |    pub suspend_count: AtomicUsize,
   60|       |    pub suspension_requested: AtomicBool,
   61|       |    pub suspended: Condvar,
   62|       |    pub active_worker_count: AtomicUsize,
   63|       |    pub suspended_worker_count: AtomicUsize,
   64|       |
   65|       |    // Collector thread notification
   66|       |    pub phase_changed: Condvar,
   67|       |    pub phase_change_mutex: Mutex<()>,
   68|       |
   69|       |    // Thread registry for cooperative root scanning
   70|       |    pub registered_threads: Mutex<Vec<ThreadRegistration>>,
   71|       |}
   72|       |
   73|       |// Thread-local state for mutators
   74|       |thread_local! {
   75|       |    pub static MUTATOR_STATE: std::cell::RefCell<MutatorState> = std::cell::RefCell::new(MutatorState::new());
   76|       |}
   77|       |
   78|       |pub struct MutatorState {
   79|       |    pub local_mark_stack: Vec<SendPtr<GcHeader<()>>>,
   80|       |    pub allocation_buffer: SegmentBuffer,
   81|       |    pub is_in_handshake: bool,
   82|       |    pub allocating_black: bool,
   83|       |}
   84|       |
   85|       |impl Default for MutatorState {
   86|      0|    fn default() -> Self {
   87|      0|        Self::new()
   88|      0|    }
   89|       |}
   90|       |
   91|       |impl MutatorState {
   92|     26|    pub fn new() -> Self {
   93|     26|        MutatorState {
   94|     26|            local_mark_stack: Vec::new(),
   95|     26|            allocation_buffer: SegmentBuffer::default(),
   96|     26|            is_in_handshake: false,
   97|     26|            allocating_black: false,
   98|     26|        }
   99|     26|    }
  100|       |
  101|  52.5k|    pub fn try_allocate<T>(&mut self) -> Option<*mut GcHeader<T>> {
  102|  52.5k|        let size = std::mem::size_of::<GcHeader<T>>();
  103|  52.5k|        let align = std::mem::align_of::<GcHeader<T>>();
  104|       |
  105|       |        // Check if we have space in our thread-local buffer
  106|  52.5k|        if self.allocation_buffer.current.is_null() {
  107|  52.5k|            return None; // No buffer allocated yet
  108|      0|        }
  109|       |
  110|      0|        let current = self.allocation_buffer.current;
  111|      0|        let aligned = align_up(current, align) as *mut u8;
  112|      0|        let new_ptr = unsafe { aligned.add(size) };
  113|       |
  114|      0|        if new_ptr <= self.allocation_buffer.end {
  115|       |            // We have space in the thread-local buffer
  116|      0|            self.allocation_buffer.current = new_ptr;
  117|      0|            Some(aligned as *mut GcHeader<T>)
  118|       |        } else {
  119|       |            // Buffer exhausted, need to get a new one
  120|      0|            None
  121|       |        }
  122|  52.5k|    }
  123|       |
  124|       |    /// Check for and participate in handshake if requested.
  125|       |    /// This should be called periodically by mutator threads,
  126|       |    /// especially during allocation-heavy operations.
  127|      0|    pub fn check_handshake(&mut self, collector: &CollectorState) {
  128|      0|        if collector.is_handshake_requested() && !self.is_in_handshake {
  129|      0|            self.is_in_handshake = true;
  130|      0|
  131|      0|            // Switch allocation color based on collector state
  132|      0|            self.allocating_black = collector.allocation_color.load(Ordering::Acquire);
  133|      0|
  134|      0|            // Acknowledge participation in the handshake
  135|      0|            collector.acknowledge_handshake();
  136|      0|
  137|      0|            self.is_in_handshake = false;
  138|      0|        }
  139|      0|    }
  140|       |}
  141|       |
  142|       |impl Default for CollectorState {
  143|      0|    fn default() -> Self {
  144|      0|        Self::new()
  145|      0|    }
  146|       |}
  147|       |
  148|       |impl CollectorState {
  149|      8|    pub fn new() -> Self {
  150|      8|        CollectorState {
  151|      8|            phase: AtomicUsize::new(CollectorPhase::Waiting as usize),
  152|      8|            marking_active: AtomicBool::new(false),
  153|      8|            allocation_color: AtomicBool::new(false),
  154|      8|            global_mark_stack: Mutex::new(Vec::new()),
  155|      8|            worker_count: AtomicUsize::new(0),
  156|      8|            workers_finished: AtomicUsize::new(0),
  157|      8|            handshake_requested: AtomicBool::new(false),
  158|      8|            handshake_completed: Condvar::new(),
  159|      8|            active_mutator_count: AtomicUsize::new(0),
  160|      8|            handshake_acknowledgments: AtomicUsize::new(0),
  161|      8|            suspend_count: AtomicUsize::new(0),
  162|      8|            suspension_requested: AtomicBool::new(false),
  163|      8|            suspended: Condvar::new(),
  164|      8|            active_worker_count: AtomicUsize::new(0),
  165|      8|            suspended_worker_count: AtomicUsize::new(0),
  166|      8|            phase_changed: Condvar::new(),
  167|      8|            phase_change_mutex: Mutex::new(()),
  168|      8|            registered_threads: Mutex::new(Vec::new()),
  169|      8|        }
  170|      8|    }
  171|       |
  172|      1|    pub fn is_marking(&self) -> bool {
  173|      1|        self.marking_active.load(Ordering::Acquire)
  174|      1|    }
  175|       |
  176|      0|    pub fn request_collection(&self) {
  177|       |        // Simple trigger - set phase to Marking if we're currently Waiting
  178|      0|        let result = self.phase.compare_exchange(
  179|      0|            CollectorPhase::Waiting as usize,
  180|      0|            CollectorPhase::Marking as usize,
  181|      0|            std::sync::atomic::Ordering::Release,
  182|      0|            std::sync::atomic::Ordering::Relaxed,
  183|       |        );
  184|       |
  185|       |        // If phase change succeeded, notify collector threads
  186|      0|        if result.is_ok() {
  187|      0|            let _guard = self.phase_change_mutex.lock().unwrap();
  188|      0|            self.phase_changed.notify_all();
  189|      0|        }
  190|      0|    }
  191|       |
  192|      1|    pub fn set_phase(&self, new_phase: CollectorPhase) {
  193|      1|        self.phase.store(new_phase as usize, Ordering::Release);
  194|      1|        let _guard = self.phase_change_mutex.lock().unwrap();
  195|      1|        self.phase_changed.notify_all();
  196|      1|    }
  197|       |
  198|      0|    pub fn wait_for_phase_change(&self, expected_phase: CollectorPhase) {
  199|      0|        let guard = self.phase_change_mutex.lock().unwrap();
  200|      0|        let _result = self.phase_changed.wait_while(guard, |_| {
  201|      0|            self.phase.load(Ordering::Acquire) == expected_phase as usize
  202|      0|        });
  203|      0|    }
  204|       |
  205|      1|    pub fn register_mutator_thread(&self) {
  206|      1|        self.active_mutator_count.fetch_add(1, Ordering::Release);
  207|      1|    }
  208|       |
  209|      1|    pub fn unregister_mutator_thread(&self) {
  210|      1|        self.active_mutator_count.fetch_sub(1, Ordering::Release);
  211|      1|    }
  212|       |
  213|      1|    pub fn get_active_mutator_count(&self) -> usize {
  214|      1|        self.active_mutator_count.load(Ordering::Acquire)
  215|      1|    }
  216|       |
  217|      0|    pub fn register_worker_thread(&self) {
  218|      0|        self.active_worker_count.fetch_add(1, Ordering::Release);
  219|      0|    }
  220|       |
  221|      0|    pub fn unregister_worker_thread(&self) {
  222|      0|        self.active_worker_count.fetch_sub(1, Ordering::Release);
  223|      0|    }
  224|       |
  225|      0|    pub fn worker_acknowledge_suspension(&self) {
  226|      0|        let prev_suspended = self.suspended_worker_count.fetch_add(1, Ordering::Release);
  227|      0|        let active_workers = self.active_worker_count.load(Ordering::Acquire);
  228|       |
  229|       |        // If this is the last worker to suspend, notify the collector
  230|      0|        if prev_suspended + 1 >= active_workers {
  231|      0|            self.suspended.notify_all();
  232|      0|        }
  233|      0|    }
  234|       |
  235|      0|    pub fn worker_acknowledge_resumption(&self) {
  236|      0|        self.suspended_worker_count.fetch_sub(1, Ordering::Release);
  237|      0|    }
  238|       |
  239|       |    /// Register a thread for cooperative garbage collection
  240|      1|    pub fn register_thread_for_gc(&self, stack_bounds: (usize, usize)) -> Result<(), &'static str> {
  241|      1|        let current_thread_id = std::thread::current().id();
  242|       |
  243|       |        // Get current stack pointer using psm
  244|      1|        let current_sp = psm::stack_pointer() as usize;
  245|       |
  246|      1|        let registration = ThreadRegistration {
  247|      1|            thread_id: current_thread_id,
  248|      1|            stack_base: stack_bounds.1, // top of stack
  249|      1|            stack_bounds,
  250|      1|            last_known_sp: AtomicUsize::new(current_sp),
  251|      1|            local_roots: Vec::new(),
  252|      1|            is_active: AtomicBool::new(true),
  253|      1|        };
  254|       |
  255|      1|        let mut threads = self.registered_threads.lock().unwrap();
  256|       |
  257|       |        // Check if thread is already registered
  258|      1|        if threads.iter().any(|t| t.thread_id == current_thread_id) {
                                                ^0             ^0
  259|      0|            return Err("Thread already registered");
  260|      1|        }
  261|       |
  262|      1|        threads.push(registration);
  263|      1|        Ok(())
  264|      1|    }
  265|       |
  266|       |    /// Unregister a thread from cooperative GC
  267|      1|    pub fn unregister_thread_from_gc(&self) {
  268|      1|        let current_thread_id = std::thread::current().id();
  269|      1|        let mut threads = self.registered_threads.lock().unwrap();
  270|      1|        threads.retain(|t| t.thread_id != current_thread_id);
  271|      1|    }
  272|       |
  273|       |    /// Update a thread's stack pointer for precise scanning
  274|      0|    pub fn update_thread_stack_pointer(&self) {
  275|      0|        let current_thread_id = std::thread::current().id();
  276|      0|        let current_sp = psm::stack_pointer() as usize;
  277|       |
  278|      0|        if let Ok(mut threads) = self.registered_threads.lock()
  279|      0|            && let Some(registration) = threads
  280|      0|                .iter_mut()
  281|      0|                .find(|t| t.thread_id == current_thread_id)
  282|      0|        {
  283|      0|            registration
  284|      0|                .last_known_sp
  285|      0|                .store(current_sp, Ordering::Release);
  286|      0|        }
  287|      0|    }
  288|       |
  289|       |    /// Get current thread's stack bounds using pthread APIs
  290|      0|    pub fn get_current_thread_stack_bounds(&self) -> (usize, usize) {
  291|       |        #[cfg(target_os = "linux")]
  292|       |        {
  293|       |            use libc::{
  294|       |                pthread_attr_getstack, pthread_attr_init, pthread_getattr_np, pthread_self,
  295|       |            };
  296|       |            use std::ptr;
  297|       |
  298|       |            unsafe {
  299|      0|                let mut attr = std::mem::zeroed();
  300|      0|                let mut stack_addr = ptr::null_mut();
  301|      0|                let mut stack_size = 0;
  302|       |
  303|      0|                if pthread_attr_init(&mut attr) == 0
  304|      0|                    && pthread_getattr_np(pthread_self(), &mut attr) == 0
  305|      0|                    && pthread_attr_getstack(&attr, &mut stack_addr, &mut stack_size) == 0
  306|       |                {
  307|      0|                    let stack_bottom = stack_addr as usize;
  308|      0|                    let stack_top = stack_bottom + stack_size;
  309|      0|                    return (stack_bottom, stack_top);
  310|      0|                }
  311|       |            }
  312|       |        }
  313|       |
  314|       |        #[cfg(target_os = "macos")]
  315|       |        {
  316|       |            unsafe {
  317|       |                let stack_base = libc::pthread_get_stackaddr_np(libc::pthread_self()) as usize;
  318|       |                let stack_size = libc::pthread_get_stacksize_np(libc::pthread_self());
  319|       |                let stack_bottom = stack_base - stack_size;
  320|       |                return (stack_bottom, stack_base);
  321|       |            }
  322|       |        }
  323|       |
  324|       |        // Fallback: return zero if platform-specific code fails
  325|      0|        (0, 0)
  326|      0|    }
  327|       |
  328|      0|    pub fn start_marking_phase(&self) {
  329|      0|        self.phase
  330|      0|            .store(CollectorPhase::Marking as usize, Ordering::Release);
  331|      0|        self.marking_active.store(true, Ordering::Release);
  332|       |
  333|       |        // Soft handshake to stop allocators and switch to black allocation
  334|      0|        self.request_handshake();
  335|      0|        self.allocation_color.store(true, Ordering::Release); // Switch to black
  336|       |
  337|       |        // Start parallel markers
  338|      0|        let worker_count = num_cpus::get();
  339|      0|        self.worker_count.store(worker_count, Ordering::Release);
  340|       |
  341|       |        // Get Arc reference to self from the global COLLECTOR
  342|       |        use crate::memory::COLLECTOR;
  343|      0|        let collector_arc = COLLECTOR.clone();
  344|       |
  345|      0|        for _ in 0..worker_count {
  346|      0|            let collector_clone = collector_arc.clone();
  347|      0|            std::thread::spawn(move || {
  348|      0|                collector_clone.marking_worker();
  349|      0|            });
  350|       |        }
  351|       |
  352|       |        // Mark roots
  353|      0|        self.mark_global_roots();
  354|      0|    }
  355|       |
  356|      0|    pub fn marking_worker(&self) {
  357|      0|        let mut local_stack: Vec<SendPtr<GcHeader<()>>> = Vec::new();
  358|       |
  359|      0|        while self.marking_active.load(Ordering::Acquire) {
  360|       |            // Check for suspension request
  361|      0|            if self.is_suspension_requested() {
  362|       |                // Acknowledge suspension and wait for resume
  363|      0|                self.worker_suspended();
  364|      0|                continue;
  365|      0|            }
  366|       |
  367|       |            // Try to get work from global stack
  368|      0|            if local_stack.is_empty() {
  369|      0|                if let Some(work) = self.steal_marking_work() {
  370|      0|                    local_stack.extend(work);
  371|      0|                } else {
  372|       |                    // No work available, yield and try again
  373|      0|                    std::thread::yield_now();
  374|      0|                    continue;
  375|       |                }
  376|      0|            }
  377|       |
  378|       |            // Process local work
  379|      0|            while let Some(header_ptr) = local_stack.pop() {
  380|       |                unsafe {
  381|      0|                    let header = &*header_ptr.as_ptr();
  382|      0|                    if !header.mark_bit.load(Ordering::Acquire) {
  383|      0|                        header.mark_bit.store(true, Ordering::Release);
  384|      0|
  385|      0|                        // Trace outgoing pointers using type info
  386|      0|                        let data_ptr = (&header.data as *const ()).cast::<()>();
  387|      0|                        (header.type_info.trace_fn)(data_ptr, &mut local_stack);
  388|      0|                    }
  389|       |                }
  390|       |
  391|       |                // Donate work back if stack gets too large
  392|      0|                if local_stack.len() > 1000 {
  393|      0|                    self.donate_marking_work(&mut local_stack);
  394|      0|                }
  395|       |            }
  396|       |        }
  397|       |
  398|       |        // Donate remaining work before finishing
  399|      0|        if !local_stack.is_empty() {
  400|      0|            self.donate_marking_work(&mut local_stack);
  401|      0|        }
  402|      0|    }
  403|       |
  404|       |    /// Steal work from the global mark stack.
  405|       |    /// Returns a batch of work items to process locally.
  406|      1|    pub fn steal_marking_work(&self) -> Option<Vec<SendPtr<GcHeader<()>>>> {
  407|       |        const STEAL_BATCH_SIZE: usize = 32;
  408|       |
  409|      1|        let mut global_stack = self.global_mark_stack.lock().unwrap();
  410|      1|        if global_stack.is_empty() {
  411|      0|            return None;
  412|      1|        }
  413|       |
  414|       |        // Steal up to STEAL_BATCH_SIZE items, or half the remaining work
  415|      1|        let steal_count = (global_stack.len() / 2).clamp(1, STEAL_BATCH_SIZE);
  416|       |
  417|      1|        let stolen_work = global_stack.drain(..steal_count).collect();
  418|      1|        Some(stolen_work)
  419|      1|    }
  420|       |
  421|       |    /// Donate work from local stack back to the global stack.
  422|       |    /// This helps balance work across all worker threads.
  423|      1|    pub fn donate_marking_work(&self, local_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  424|       |        const DONATION_BATCH_SIZE: usize = 100;
  425|       |
  426|      1|        if local_stack.len() <= DONATION_BATCH_SIZE {
  427|      0|            return; // Not enough work to donate
  428|      1|        }
  429|       |
  430|       |        // Donate half of the local work to the global stack
  431|      1|        let donate_count = local_stack.len() / 2;
  432|      1|        let mut donated_work = local_stack.drain(..donate_count).collect::<Vec<_>>();
  433|       |
  434|      1|        let mut global_stack = self.global_mark_stack.lock().unwrap();
  435|      1|        global_stack.append(&mut donated_work);
  436|      1|    }
  437|       |
  438|       |    /// Request a handshake with all mutator threads.
  439|       |    /// This is used to coordinate GC phases and ensure all mutators
  440|       |    /// switch to the appropriate allocation mode.
  441|      0|    pub fn request_handshake(&self) {
  442|       |        // Signal that a handshake is requested
  443|      0|        self.handshake_requested.store(true, Ordering::Release);
  444|       |
  445|       |        // Wait for all active mutators to acknowledge the handshake
  446|      0|        let active_mutators = self.active_mutator_count.load(Ordering::Acquire);
  447|      0|        self.handshake_acknowledgments.store(0, Ordering::Release);
  448|       |
  449|      0|        if active_mutators > 0 {
  450|      0|            let guard = self.global_mark_stack.lock().unwrap();
  451|      0|            let _result = self.handshake_completed.wait_timeout_while(
  452|      0|                guard,
  453|      0|                std::time::Duration::from_millis(100),
  454|      0|                |_| self.handshake_acknowledgments.load(Ordering::Acquire) < active_mutators,
  455|       |            );
  456|      0|        }
  457|       |
  458|       |        // Clear the handshake request
  459|      0|        self.handshake_requested.store(false, Ordering::Release);
  460|      0|    }
  461|       |
  462|       |    /// Check if a handshake is currently requested.
  463|       |    /// Mutator threads should call this periodically and respond by
  464|       |    /// calling acknowledge_handshake().
  465|      0|    pub fn is_handshake_requested(&self) -> bool {
  466|      0|        self.handshake_requested.load(Ordering::Acquire)
  467|      0|    }
  468|       |
  469|       |    /// Acknowledge participation in the current handshake.
  470|       |    /// This should be called by mutator threads when they detect
  471|       |    /// a handshake request and have completed their synchronization.
  472|      0|    pub fn acknowledge_handshake(&self) {
  473|       |        // Increment the acknowledgment counter
  474|      0|        let prev_acks = self
  475|      0|            .handshake_acknowledgments
  476|      0|            .fetch_add(1, Ordering::Release);
  477|      0|        let active_mutators = self.active_mutator_count.load(Ordering::Acquire);
  478|       |
  479|       |        // If this is the last acknowledgment, notify the collector
  480|      0|        if prev_acks + 1 >= active_mutators {
  481|      0|            self.handshake_completed.notify_all();
  482|      0|        }
  483|      0|    }
  484|       |
  485|       |    /// Mark all global roots to start the marking phase.
  486|       |    /// This includes static variables, thread stacks, and other root sources.
  487|      0|    pub fn mark_global_roots(&self) {
  488|      0|        let mut global_stack = self.global_mark_stack.lock().unwrap();
  489|       |
  490|       |        // Mark static/global roots
  491|      0|        self.mark_static_roots(&mut global_stack);
  492|       |
  493|       |        // Mark thread stack roots
  494|      0|        self.mark_thread_stacks(&mut global_stack);
  495|       |
  496|       |        // Mark thread-local storage roots
  497|      0|        self.mark_thread_locals(&mut global_stack);
  498|       |
  499|       |        // Mark other system roots (e.g., JIT code, finalizer queues, etc.)
  500|      0|        self.mark_system_roots(&mut global_stack);
  501|      0|    }
  502|       |
  503|       |    /// Mark roots from static/global variables.
  504|       |    /// This scans the data segment for potential GC pointers.
  505|      0|    fn mark_static_roots(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  506|       |        // Conservative scanning of static data segment
  507|       |        // Scan .data and .bss segments for potential GC pointers
  508|      0|        unsafe {
  509|      0|            self.scan_data_segment(global_stack);
  510|      0|            self.scan_bss_segment(global_stack);
  511|      0|        }
  512|      0|    }
  513|       |
  514|      0|    unsafe fn scan_data_segment(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  515|       |        // Get data segment bounds using linker symbols (Linux/ELF specific)
  516|       |        #[cfg(target_os = "linux")]
  517|       |        {
  518|       |            unsafe extern "C" {
  519|       |                static mut __data_start: u8;
  520|       |                static mut _edata: u8;
  521|       |            }
  522|       |
  523|      0|            let data_start = &raw const __data_start;
  524|      0|            let data_end = &raw const _edata;
  525|       |
  526|      0|            if data_start < data_end {
  527|      0|                unsafe {
  528|      0|                    self.conservative_scan_memory_range(data_start, data_end, global_stack);
  529|      0|                }
  530|      0|            }
  531|       |        }
  532|       |
  533|       |        #[cfg(not(target_os = "linux"))]
  534|       |        {
  535|       |            // Fallback: scan a conservative estimate of global variables
  536|       |            // On other platforms, we'd use platform-specific APIs
  537|       |            // For now, just scan a few pages around likely static data
  538|       |            let dummy_static: u8 = 0;
  539|       |            let estimate_start = &dummy_static as *const u8;
  540|       |            let estimate_size = 64 * 1024; // 64KB conservative estimate
  541|       |            let estimate_end = estimate_start.add(estimate_size);
  542|       |
  543|       |            self.conservative_scan_memory_range(estimate_start, estimate_end, global_stack);
  544|       |        }
  545|      0|    }
  546|       |
  547|      0|    unsafe fn scan_bss_segment(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  548|       |        // Get BSS segment bounds using linker symbols (Linux/ELF specific)
  549|       |        #[cfg(target_os = "linux")]
  550|       |        {
  551|       |            unsafe extern "C" {
  552|       |                static mut __bss_start: u8;
  553|       |                static mut _end: u8;
  554|       |            }
  555|       |
  556|      0|            let bss_start = &raw const __bss_start;
  557|      0|            let bss_end = &raw const _end;
  558|       |
  559|      0|            if bss_start < bss_end {
  560|      0|                unsafe {
  561|      0|                    self.conservative_scan_memory_range(bss_start, bss_end, global_stack);
  562|      0|                }
  563|      0|            }
  564|       |        }
  565|       |
  566|       |        #[cfg(not(target_os = "linux"))]
  567|       |        {
  568|       |            // Fallback for non-Linux platforms - BSS scanning would require
  569|       |            // platform-specific implementations using GetModuleInformation (Windows),
  570|       |            // _dyld_image_count/_dyld_get_image_header (macOS), etc.
  571|       |        }
  572|      0|    }
  573|       |
  574|       |    /// Mark roots from thread stacks.
  575|       |    /// This performs conservative scanning of all thread stacks.
  576|      0|    fn mark_thread_stacks(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  577|       |        // Conservative stack scanning
  578|       |        // Start with the current thread's stack
  579|      0|        self.mark_current_thread_stack(global_stack);
  580|       |
  581|       |        // Enumerate and scan other threads' stacks
  582|      0|        self.mark_all_thread_stacks(global_stack);
  583|      0|    }
  584|       |
  585|      0|    fn mark_all_thread_stacks(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  586|       |        // Cross-platform implementation using robust stack introspection libraries
  587|       |        // This replaces the fragile /proc parsing with proper stack bounds detection
  588|       |
  589|       |        // Scan current thread using psm crate for reliable stack pointer detection
  590|      0|        self.mark_current_thread_stack_with_psm(global_stack);
  591|       |
  592|       |        // For multi-threaded scanning, we use a cooperative approach rather than
  593|       |        // trying to introspect arbitrary threads (which is inherently unsafe)
  594|      0|        self.mark_registered_thread_roots(global_stack);
  595|      0|    }
  596|       |
  597|       |    /// Scan current thread's stack using the psm crate for precise stack pointer detection
  598|      0|    fn mark_current_thread_stack_with_psm(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  599|       |        // Use psm to get the current stack pointer
  600|      0|        let current_sp = psm::stack_pointer() as *const u8;
  601|       |
  602|       |        // Get stack bounds using platform-specific pthread APIs (more reliable than /proc)
  603|      0|        let (stack_bottom, stack_top) = self.get_current_thread_stack_bounds();
  604|      0|        let stack_base = stack_top as *const u8;
  605|       |
  606|      0|        if stack_top != 0 && stack_bottom != 0 {
  607|       |            // Scan from current stack pointer to stack base
  608|      0|            let scan_start = if current_sp > (stack_bottom as *const u8) && current_sp < stack_base
  609|       |            {
  610|      0|                current_sp // Start from current position
  611|       |            } else {
  612|      0|                stack_bottom as *const u8 // Fallback to stack bottom
  613|       |            };
  614|       |
  615|      0|            unsafe {
  616|      0|                self.conservative_scan_memory_range(scan_start, stack_base, global_stack);
  617|      0|            }
  618|      0|        }
  619|      0|    }
  620|       |
  621|       |    /// Mark roots from threads that have registered themselves cooperatively
  622|      0|    fn mark_registered_thread_roots(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  623|       |        // Scan all registered threads cooperatively using their registered stack bounds
  624|      0|        if let Ok(threads) = self.registered_threads.lock() {
  625|      0|            for registration in threads.iter() {
  626|       |                // Only scan active threads
  627|      0|                if !registration
  628|      0|                    .is_active
  629|      0|                    .load(std::sync::atomic::Ordering::Acquire)
  630|       |                {
  631|      0|                    continue;
  632|      0|                }
  633|       |
  634|       |                // Get the thread's last known stack pointer
  635|      0|                let last_sp = registration
  636|      0|                    .last_known_sp
  637|      0|                    .load(std::sync::atomic::Ordering::Acquire);
  638|       |
  639|      0|                if last_sp != 0
  640|      0|                    && registration.stack_bounds.0 != 0
  641|      0|                    && registration.stack_bounds.1 != 0
  642|       |                {
  643|       |                    // Scan from last known stack pointer to stack base
  644|      0|                    let scan_start = if last_sp >= registration.stack_bounds.0
  645|      0|                        && last_sp <= registration.stack_bounds.1
  646|       |                    {
  647|      0|                        last_sp as *const u8
  648|       |                    } else {
  649|      0|                        registration.stack_bounds.0 as *const u8 // fallback to stack bottom
  650|       |                    };
  651|       |
  652|       |                    // Conservative scan of this thread's stack
  653|      0|                    unsafe {
  654|      0|                        self.conservative_scan_memory_range(
  655|      0|                            scan_start,
  656|      0|                            registration.stack_bounds.1 as *const u8, // scan to stack top
  657|      0|                            global_stack,
  658|      0|                        );
  659|      0|                    }
  660|      0|                }
  661|       |
  662|       |                // Also add any explicitly registered local roots
  663|      0|                global_stack.extend(registration.local_roots.iter().copied());
  664|       |            }
  665|      0|        }
  666|       |
  667|       |        // Additionally, scan thread-local mark stacks from all mutator threads
  668|       |        // This uses the existing thread-local MUTATOR_STATE infrastructure
  669|       |
  670|       |        // Note: We can only scan the current thread's local stack directly
  671|       |        // Other threads' local stacks would be scanned when they cooperate during handshake
  672|      0|        MUTATOR_STATE.with(|state| {
  673|      0|            let state = state.borrow();
  674|      0|            global_stack.extend(state.local_mark_stack.iter().copied());
  675|      0|        });
  676|      0|    }
  677|       |
  678|       |    /// Mark roots from thread-local storage.
  679|      0|    fn mark_thread_locals(&self, _global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  680|       |        // Scan thread-local variables that might contain GC pointers
  681|       |        // This would iterate through all threads and scan their TLS
  682|       |
  683|       |        // Example: scan mutator state local mark stacks
  684|       |        // MUTATOR_STATE.with(|state| {
  685|       |        //     let state = state.borrow();
  686|       |        //     for ptr in &state.local_mark_stack {
  687|       |        //         global_stack.push(*ptr);
  688|       |        //     }
  689|       |        // });
  690|      0|    }
  691|       |
  692|       |    /// Mark other system roots like finalizer queues, JIT code, etc.
  693|      0|    fn mark_system_roots(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  694|       |        // Mark finalizable objects that are still reachable
  695|       |        // Mark weak reference targets that should remain alive
  696|       |        // Mark any JIT-generated code that references GC objects
  697|       |
  698|       |        // For our implementation, we'll mark objects in the classified allocator
  699|       |        use crate::memory::CLASSIFIED_ALLOCATOR;
  700|       |
  701|       |        // Mark finalizable objects - these are roots during finalization
  702|       |        // ObjectClass is already imported at module level
  703|      0|        let finalizable_set = CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::Finalizer);
  704|       |
  705|       |        // Iterate through finalizable objects and mark those that are reachable
  706|      0|        let objects = finalizable_set.objects.read();
  707|      0|        for obj_ptr in objects.iter() {
  708|       |            unsafe {
  709|      0|                let header = &*obj_ptr.as_ptr();
  710|       |                // If the object is already marked during regular marking, include its finalizer
  711|      0|                if header.mark_bit.load(Ordering::Acquire) {
  712|      0|                    // This object is reachable, so its finalizer should be considered a root
  713|      0|                    // Add it to the mark stack to ensure finalization graph is traced
  714|      0|                    global_stack.push(*obj_ptr);
  715|      0|                }
  716|       |            }
  717|       |        }
  718|      0|    }
  719|       |
  720|       |    /// Conservative memory scanning for a memory range.
  721|       |    /// Scans memory treating each word as a potential pointer.
  722|       |    #[allow(dead_code)]
  723|      0|    unsafe fn conservative_scan_memory_range(
  724|      0|        &self,
  725|      0|        start: *const u8,
  726|      0|        end: *const u8,
  727|      0|        global_stack: &mut Vec<SendPtr<GcHeader<()>>>,
  728|      0|    ) {
  729|      0|        let mut current = start as *const usize;
  730|      0|        let end_ptr = end as *const usize;
  731|       |
  732|      0|        while current < end_ptr {
  733|      0|            let potential_ptr = unsafe { *current };
  734|       |
  735|       |            // Check if this looks like a valid GC pointer
  736|      0|            if self.is_valid_gc_pointer(potential_ptr as *mut GcHeader<()>) {
  737|      0|                global_stack.push(unsafe { SendPtr::new(potential_ptr as *mut GcHeader<()>) });
  738|      0|            }
  739|       |
  740|      0|            current = unsafe { current.add(1) };
  741|       |        }
  742|      0|    }
  743|       |
  744|       |    /// Check if a potential pointer is a valid GC object.
  745|       |    /// This performs basic sanity checks to avoid false positives.
  746|      0|    fn is_valid_gc_pointer(&self, ptr: *mut GcHeader<()>) -> bool {
  747|       |        // Comprehensive pointer validation
  748|      0|        if ptr.is_null() {
  749|      0|            return false;
  750|      0|        }
  751|       |
  752|       |        // Check proper alignment for GcHeader
  753|      0|        if !(ptr as usize).is_multiple_of(std::mem::align_of::<GcHeader<()>>()) {
  754|      0|            return false;
  755|      0|        }
  756|       |
  757|       |        // Check if the pointer is within heap bounds
  758|      0|        if !self.is_within_heap_bounds(ptr) {
  759|      0|            return false;
  760|      0|        }
  761|       |
  762|       |        // Validate the TypeInfo structure
  763|       |        unsafe {
  764|      0|            let header = &*ptr;
  765|      0|            if !self.is_valid_type_info(header.type_info) {
  766|      0|                return false;
  767|      0|            }
  768|       |
  769|       |            // Check if the object size is reasonable
  770|      0|            let size = header.type_info.size;
  771|      0|            if size < std::mem::size_of::<GcHeader<()>>() || size > 64 * 1024 * 1024 {
  772|      0|                return false;
  773|      0|            }
  774|       |
  775|       |            // Check if the object is within segment boundaries
  776|      0|            self.is_valid_object_in_segment(ptr, size)
  777|       |        }
  778|      0|    }
  779|       |
  780|      0|    fn is_within_heap_bounds(&self, ptr: *mut GcHeader<()>) -> bool {
  781|       |        use crate::memory::ALLOCATOR;
  782|       |
  783|       |        // Check if the pointer falls within any of our heap segments
  784|      0|        let segments = ALLOCATOR.get_heap().segments.lock().unwrap();
  785|       |
  786|      0|        for segment in segments.iter() {
  787|      0|            let segment_start = segment.memory.as_ptr() as *mut u8;
  788|      0|            let segment_end = segment.end_ptr.load(Ordering::Relaxed);
  789|       |
  790|      0|            if (ptr as *mut u8) >= segment_start && (ptr as *mut u8) < segment_end {
  791|      0|                return true;
  792|      0|            }
  793|       |        }
  794|       |
  795|      0|        false
  796|      0|    }
  797|       |
  798|      0|    unsafe fn is_valid_type_info(&self, type_info: &TypeInfo) -> bool {
  799|       |        // Basic TypeInfo validation
  800|       |        // Function pointers are not optional in our design, so they're always valid
  801|       |
  802|       |        // Check that size is reasonable
  803|      0|        if type_info.size == 0 || type_info.size > 64 * 1024 * 1024 {
  804|      0|            return false;
  805|      0|        }
  806|       |
  807|       |        // Additional validation could include:
  808|       |        // - Checking that function pointers point to valid code
  809|       |        // - Verifying object type signatures
  810|       |        // - Checking type registration tables
  811|       |
  812|      0|        true
  813|      0|    }
  814|       |
  815|      0|    fn is_valid_object_in_segment(&self, ptr: *mut GcHeader<()>, size: usize) -> bool {
  816|       |        use crate::memory::ALLOCATOR;
  817|       |
  818|      0|        let segments = ALLOCATOR.get_heap().segments.lock().unwrap();
  819|      0|        let ptr_addr = ptr as *mut u8;
  820|       |
  821|      0|        for segment in segments.iter() {
  822|      0|            let segment_start = segment.memory.as_ptr() as *mut u8;
  823|      0|            let segment_end = segment.end_ptr.load(Ordering::Relaxed);
  824|       |
  825|       |            // Check if object starts within this segment
  826|      0|            if ptr_addr >= segment_start && ptr_addr < segment_end {
  827|       |                // Check if entire object fits within segment
  828|      0|                let object_end = unsafe { ptr_addr.add(size) };
  829|      0|                return object_end <= segment_end;
  830|      0|            }
  831|       |        }
  832|       |
  833|      0|        false
  834|      0|    }
  835|       |
  836|       |    /// Called by worker threads to acknowledge suspension and wait for resume.
  837|      0|    pub fn worker_suspended(&self) {
  838|       |        // Acknowledge suspension
  839|      0|        self.worker_acknowledge_suspension();
  840|       |
  841|       |        // Wait for suspension to be cleared
  842|      0|        let guard = self.global_mark_stack.lock().unwrap();
  843|      0|        let _result = self
  844|      0|            .suspended
  845|      0|            .wait_while(guard, |_| self.suspension_requested.load(Ordering::Acquire));
  846|       |
  847|       |        // Acknowledge resumption
  848|      0|        self.worker_acknowledge_resumption();
  849|      0|    }
  850|       |
  851|       |    /// Mark roots from the current thread's stack using conservative scanning.
  852|       |    /// This scans the stack from the current frame down to the stack base.
  853|      0|    fn mark_current_thread_stack(&self, global_stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  854|       |        // Get approximate stack bounds for the current thread
  855|      0|        let (stack_bottom_addr, stack_top_addr) = self.get_current_thread_stack_bounds();
  856|      0|        let stack_base = stack_bottom_addr as *const u8;
  857|      0|        let stack_top = stack_top_addr as *const u8;
  858|       |
  859|      0|        if !stack_base.is_null() && !stack_top.is_null() && stack_top <= stack_base {
  860|      0|            unsafe {
  861|      0|                self.conservative_scan_memory_range(stack_top, stack_base, global_stack);
  862|      0|            }
  863|      0|        }
  864|      0|    }
  865|       |
  866|       |    // Fork safety methods from suspend_for_fork.rs
  867|      0|    pub fn suspend_for_fork(&self) {
  868|      0|        let suspend_count = self.suspend_count.fetch_add(1, Ordering::AcqRel);
  869|       |
  870|      0|        if suspend_count == 0 {
  871|      0|            // First suspension - actually suspend the collector
  872|      0|            self.request_suspension();
  873|      0|
  874|      0|            // Wait for all collector threads and workers to stop
  875|      0|            self.wait_for_suspension();
  876|      0|        }
  877|      0|    }
  878|       |
  879|      0|    pub fn resume_after_fork(&self) {
  880|      0|        let suspend_count = self.suspend_count.fetch_sub(1, Ordering::AcqRel);
  881|       |
  882|      0|        if suspend_count == 1 {
  883|      0|            // Last resume - restart the collector
  884|      0|            self.resume_collection();
  885|      0|        }
  886|      0|    }
  887|       |
  888|       |    /// Request suspension of all GC activities.
  889|       |    /// This signals all collector threads to suspend their operations.
  890|      0|    fn request_suspension(&self) {
  891|       |        // Signal suspension request
  892|      0|        self.suspension_requested.store(true, Ordering::Release);
  893|       |
  894|       |        // Stop marking phase if it's active
  895|      0|        self.marking_active.store(false, Ordering::Release);
  896|      0|    }
  897|       |
  898|       |    /// Wait for all GC activities to be suspended.
  899|       |    /// This blocks until all collector threads have acknowledged suspension.
  900|      0|    fn wait_for_suspension(&self) {
  901|       |        // Reset suspended worker count
  902|      0|        self.suspended_worker_count.store(0, Ordering::Release);
  903|       |
  904|       |        // Wait for all active workers to acknowledge suspension
  905|      0|        let active_workers = self.active_worker_count.load(Ordering::Acquire);
  906|       |
  907|      0|        if active_workers > 0 {
  908|      0|            let guard = self.global_mark_stack.lock().unwrap();
  909|       |
  910|       |            // Wait with timeout for all workers to suspend
  911|      0|            let _result = self.suspended.wait_timeout_while(
  912|      0|                guard,
  913|      0|                std::time::Duration::from_millis(1000), // Increased timeout
  914|      0|                |_| self.suspended_worker_count.load(Ordering::Acquire) < active_workers,
  915|       |            );
  916|       |
  917|       |            // If timeout occurred, log a warning but continue
  918|       |            // In production, we might want to be more aggressive
  919|      0|            let suspended_count = self.suspended_worker_count.load(Ordering::Acquire);
  920|      0|            if suspended_count < active_workers {
  921|      0|                eprintln!(
  922|      0|                    "Warning: Only {}/{} worker threads acknowledged suspension",
  923|      0|                    suspended_count, active_workers
  924|      0|                );
  925|      0|            }
  926|      0|        }
  927|      0|    }
  928|       |
  929|       |    /// Resume collection after suspension.
  930|       |    /// This allows GC activities to restart.
  931|      0|    fn resume_collection(&self) {
  932|       |        // Clear the suspension request
  933|      0|        self.suspension_requested.store(false, Ordering::Release);
  934|       |
  935|       |        // Notify any threads waiting for resume
  936|      0|        self.suspended.notify_all();
  937|      0|    }
  938|       |
  939|       |    /// Check if suspension is currently requested.
  940|       |    /// Worker threads should check this periodically and suspend if needed.
  941|      0|    pub fn is_suspension_requested(&self) -> bool {
  942|      0|        self.suspension_requested.load(Ordering::Acquire)
  943|      0|    }
  944|       |
  945|       |    // Sweeping phase methods from sweeping_phase.rs
  946|      0|    pub fn sweeping_phase(&self) {
  947|      0|        self.phase
  948|      0|            .store(CollectorPhase::Sweeping as usize, Ordering::Release);
  949|       |
  950|      0|        let _free_singleton = FreeSingleton::instance();
  951|       |
  952|       |        // Parallel sweep with redirection
  953|      0|        self.sweep_all_segments_parallel(_free_singleton);
  954|      0|    }
  955|       |
  956|       |    /// Redirects all pointers to a dead object to point to the free singleton instead.
  957|       |    ///
  958|       |    /// # Safety
  959|       |    ///
  960|       |    /// This function is unsafe because:
  961|       |    /// - Both pointers must be valid and properly aligned
  962|       |    /// - `dead_obj` must point to an object that has been determined to be unreachable
  963|       |    /// - `free_singleton` must point to the valid free singleton object
  964|       |    /// - The caller must ensure no other threads are accessing the dead object
  965|       |    /// - This should only be called during the sweep phase of garbage collection
  966|      0|    pub unsafe fn redirect_pointers_to_free_singleton(
  967|      0|        &self,
  968|      0|        dead_obj: *mut GcHeader<()>,
  969|      0|        free_singleton: *mut GcHeader<()>,
  970|      0|    ) {
  971|       |        // This is the key innovation - redirect pointers to dead objects
  972|       |        // In practice, this requires scanning all live objects and updating their pointers
  973|       |        // FUGC does this efficiently by maintaining pointer maps or using conservative scanning
  974|       |
  975|      0|        self.scan_all_live_objects(|live_obj| unsafe {
  976|      0|            let header = &*live_obj;
  977|      0|            (header.type_info.redirect_pointers_fn)(live_obj, dead_obj, free_singleton);
  978|      0|        });
  979|      0|    }
  980|       |
  981|       |    /// Sweep all segments in parallel, freeing unmarked objects.
  982|      0|    pub fn sweep_all_segments_parallel(&self, _free_singleton: *mut GcHeader<()>) {
  983|       |        use crate::memory::ALLOCATOR;
  984|       |        use rayon::prelude::*;
  985|       |
  986|       |        // Get segments from the global allocator's heap
  987|      0|        let segments = ALLOCATOR.get_heap().segments.lock().unwrap();
  988|       |
  989|       |        // Process segments in parallel
  990|       |        // Note: We get the free_singleton inside each thread to avoid sharing raw pointers
  991|      0|        segments.par_iter().for_each(|segment| {
  992|      0|            let free_singleton = FreeSingleton::instance();
  993|      0|            self.sweep_segment(segment, free_singleton);
  994|      0|        });
  995|      0|    }
  996|       |
  997|       |    /// Sweep a single segment, processing all objects within it.
  998|      0|    fn sweep_segment(&self, segment: &crate::memory::Segment, free_singleton: *mut GcHeader<()>) {
  999|      0|        let mut current_ptr = segment.memory.as_ptr() as *mut u8;
 1000|      0|        let end_ptr = segment.end_ptr.load(Ordering::Relaxed);
 1001|       |
 1002|      0|        while current_ptr < end_ptr {
 1003|      0|            let header = current_ptr as *mut GcHeader<()>;
 1004|       |
 1005|       |            unsafe {
 1006|       |                // Check if this is a valid object header
 1007|      0|                if self.is_valid_object_header(header) {
 1008|      0|                    if !(*header).mark_bit.load(Ordering::Acquire) {
 1009|       |                        // Object is dead - invalidate all weak references first
 1010|      0|                        let weak_head = (*header).weak_ref_list.load(Ordering::Acquire);
 1011|      0|                        if !weak_head.is_null() {
 1012|      0|                            // Weak is already imported at module level
 1013|      0|                            Weak::<()>::invalidate_weak_chain(weak_head);
 1014|      0|                        }
 1015|       |
 1016|       |                        // Redirect all pointers to it
 1017|      0|                        self.redirect_pointers_to_free_singleton(header, free_singleton);
 1018|       |
 1019|       |                        // Run destructor if needed
 1020|      0|                        ((*header).type_info.drop_fn)(header);
 1021|       |
 1022|       |                        // Mark as free by setting forwarding pointer
 1023|      0|                        (*header)
 1024|      0|                            .forwarding_ptr
 1025|      0|                            .store(free_singleton, Ordering::Release);
 1026|      0|                    } else {
 1027|      0|                        // Clear mark bit for next cycle
 1028|      0|                        (*header).mark_bit.store(false, Ordering::Release);
 1029|      0|                    }
 1030|       |
 1031|       |                    // Advance to next object
 1032|      0|                    let obj_size = (*header).type_info.size;
 1033|      0|                    current_ptr = current_ptr.add(obj_size);
 1034|      0|                } else {
 1035|      0|                    // Invalid header, skip ahead by pointer size to avoid infinite loop
 1036|      0|                    current_ptr = current_ptr.add(std::mem::size_of::<*mut u8>());
 1037|      0|                }
 1038|       |            }
 1039|       |        }
 1040|      0|    }
 1041|       |
 1042|       |    /// Check if a pointer points to a valid object header.
 1043|       |    /// This prevents crashes from corrupted memory or alignment issues.
 1044|      0|    fn is_valid_object_header(&self, header: *mut GcHeader<()>) -> bool {
 1045|       |        // Basic validation checks
 1046|      0|        if header.is_null() {
 1047|      0|            return false;
 1048|      0|        }
 1049|       |
 1050|       |        // Check pointer alignment
 1051|      0|        if !(header as usize).is_multiple_of(std::mem::align_of::<GcHeader<()>>()) {
 1052|      0|            return false;
 1053|      0|        }
 1054|       |
 1055|       |        unsafe {
 1056|       |            // Check if TypeInfo looks reasonable by checking size field
 1057|      0|            let type_info = (*header).type_info;
 1058|      0|            let size = type_info.size;
 1059|       |
 1060|       |            // Check if size is reasonable (between min object size and max segment size)
 1061|      0|            if size < std::mem::size_of::<GcHeader<()>>() || size > 1024 * 1024 {
 1062|      0|                return false;
 1063|      0|            }
 1064|       |        }
 1065|       |
 1066|      0|        true
 1067|      0|    }
 1068|       |
 1069|       |    /// Scan all live objects in the heap and apply a function to each one.
 1070|       |    /// This is used for pointer redirection - finding all live objects that might
 1071|       |    /// contain pointers to dead objects and updating those pointers.
 1072|      0|    pub fn scan_all_live_objects<F>(&self, mut visitor: F)
 1073|      0|    where
 1074|      0|        F: FnMut(*mut GcHeader<()>),
 1075|       |    {
 1076|       |        use crate::memory::ALLOCATOR;
 1077|       |
 1078|       |        // Get segments from the global allocator's heap
 1079|      0|        let segments = ALLOCATOR.get_heap().segments.lock().unwrap();
 1080|       |
 1081|       |        // Scan each segment for live objects
 1082|      0|        for segment in segments.iter() {
 1083|      0|            self.scan_segment_for_live_objects(segment, &mut visitor);
 1084|      0|        }
 1085|      0|    }
 1086|       |
 1087|       |    /// Scan a single segment for live objects and apply the visitor function to each.
 1088|      0|    fn scan_segment_for_live_objects<F>(&self, segment: &crate::memory::Segment, visitor: &mut F)
 1089|      0|    where
 1090|      0|        F: FnMut(*mut GcHeader<()>),
 1091|       |    {
 1092|      0|        let mut current_ptr = segment.memory.as_ptr() as *mut u8;
 1093|      0|        let end_ptr = segment.end_ptr.load(Ordering::Relaxed);
 1094|       |
 1095|      0|        while current_ptr < end_ptr {
 1096|      0|            let header = current_ptr as *mut GcHeader<()>;
 1097|       |
 1098|       |            unsafe {
 1099|       |                // Check if this is a valid object header
 1100|      0|                if self.is_valid_object_header(header) {
 1101|       |                    // Check if the object is marked (alive)
 1102|      0|                    if (*header).mark_bit.load(Ordering::Acquire) {
 1103|      0|                        // This is a live object - apply the visitor function
 1104|      0|                        visitor(header);
 1105|      0|                    }
 1106|       |
 1107|       |                    // Advance to next object
 1108|      0|                    let obj_size = (*header).type_info.size;
 1109|      0|                    current_ptr = current_ptr.add(obj_size);
 1110|      0|                } else {
 1111|      0|                    // Invalid header, skip ahead by pointer size to avoid infinite loop
 1112|      0|                    current_ptr = current_ptr.add(std::mem::size_of::<*mut u8>());
 1113|      0|                }
 1114|       |            }
 1115|       |        }
 1116|      0|    }
 1117|       |
 1118|       |    // Finalizer/reviving phase methods (from finalizable.rs)
 1119|      0|    pub fn reviving_phase(&self) {
 1120|      0|        self.phase
 1121|      0|            .store(CollectorPhase::Reviving as usize, Ordering::Release);
 1122|       |
 1123|       |        // Get finalizer set from the classified allocator
 1124|       |        use crate::memory::CLASSIFIED_ALLOCATOR;
 1125|       |        // ObjectClass is already imported at module level
 1126|      0|        let finalizer_set = CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::Finalizer);
 1127|      0|        let mut revival_stack = Vec::new();
 1128|       |
 1129|       |        // Find unmarked objects with finalizers and revive them
 1130|      0|        self.iterate_finalizer_objects_parallel(finalizer_set, &mut revival_stack);
 1131|       |
 1132|       |        // Push revived objects to mark stack for remarking phase
 1133|      0|        let revival_stack_wrapped: Vec<SendPtr<GcHeader<()>>> =
 1134|      0|            revival_stack.into_iter().map(|ptr| unsafe { SendPtr::new(ptr) }).collect();
 1135|      0|        self.global_mark_stack
 1136|      0|            .lock()
 1137|      0|            .unwrap()
 1138|      0|            .extend(revival_stack_wrapped);
 1139|      0|    }
 1140|       |
 1141|      0|    pub fn remarking_phase(&self) {
 1142|       |        // Mark any objects reachable from revived finalizable objects
 1143|       |        // This runs without handshakes since revived objects are quarantined
 1144|      0|        self.phase
 1145|      0|            .store(CollectorPhase::Remarking as usize, Ordering::Release);
 1146|       |
 1147|      0|        while !self.global_mark_stack.lock().unwrap().is_empty() {
 1148|      0|            let worker_count = num_cpus::get();
 1149|      0|            self.worker_count.store(worker_count, Ordering::Release);
 1150|       |
 1151|       |            // Get Arc reference to self from the global COLLECTOR
 1152|       |            use crate::memory::COLLECTOR;
 1153|      0|            let collector_arc = COLLECTOR.clone();
 1154|       |
 1155|      0|            for _ in 0..worker_count {
 1156|      0|                let collector_clone = collector_arc.clone();
 1157|      0|                std::thread::spawn(move || {
 1158|      0|                    collector_clone.marking_worker();
 1159|      0|                });
 1160|       |            }
 1161|       |
 1162|       |            // Wait for workers to finish by yielding and checking stack
 1163|      0|            while !self.global_mark_stack.lock().unwrap().is_empty() {
 1164|      0|                std::thread::yield_now();
 1165|      0|            }
 1166|       |        }
 1167|      0|    }
 1168|       |
 1169|       |    /// Iterate over finalizer objects in parallel to find unmarked ones for revival.
 1170|      0|    pub fn iterate_finalizer_objects_parallel(
 1171|      0|        &self,
 1172|      0|        finalizer_set: &crate::memory::ObjectSet,
 1173|      0|        revival_stack: &mut Vec<*mut GcHeader<()>>,
 1174|      0|    ) {
 1175|       |        use std::sync::{Arc, Mutex};
 1176|       |
 1177|      0|        let revival_stack_shared = Arc::new(Mutex::new(Vec::<SendPtr<GcHeader<()>>>::new()));
 1178|      0|        let worker_count = num_cpus::get();
 1179|       |
 1180|       |        // Use the ObjectSet's parallel iteration method
 1181|      0|        let stack_clone = revival_stack_shared.clone();
 1182|      0|        finalizer_set.iterate_parallel(worker_count, move |obj_ptr| {
 1183|       |            unsafe {
 1184|      0|                let header = &*obj_ptr.as_ptr();
 1185|       |
 1186|       |                // Check if the object is unmarked (needs finalization)
 1187|      0|                if !header.mark_bit.load(Ordering::Acquire) {
 1188|      0|                    // Object needs finalization - revive it
 1189|      0|                    header.mark_bit.store(true, Ordering::Release);
 1190|      0|
 1191|      0|                    // Add to the shared revival stack using SendPtr for thread safety
 1192|      0|                    let mut stack = stack_clone.lock().unwrap();
 1193|      0|                    stack.push(obj_ptr);
 1194|      0|                }
 1195|       |            }
 1196|      0|        });
 1197|       |
 1198|       |        // Merge results back to the main revival stack, converting SendPtr back to raw pointers
 1199|      0|        let shared_stack = Arc::try_unwrap(revival_stack_shared)
 1200|      0|            .unwrap_or_else(|_| panic!("Multiple references to revival_stack_shared"))
 1201|      0|            .into_inner()
 1202|      0|            .unwrap();
 1203|      0|        revival_stack.extend(shared_stack.into_iter().map(|send_ptr| send_ptr.as_ptr()));
 1204|      0|    }
 1205|       |}
 1206|       |
 1207|       |// AllocatorTrait is now imported from interfaces module
 1208|       |
 1209|       |// Safe fork wrapper (from suspend_for_fork.rs)
 1210|       |/// Safely forks the current process while suspending garbage collection.
 1211|       |///
 1212|       |/// This function ensures that garbage collection is properly suspended before
 1213|       |/// forking and resumed afterward, preventing issues with shared GC state
 1214|       |/// between parent and child processes.
 1215|       |///
 1216|       |/// # Examples
 1217|       |///
 1218|       |/// ```no_run
 1219|       |/// use fugrip::collector_phases::gc_safe_fork;
 1220|       |///
 1221|       |/// match gc_safe_fork() {
 1222|       |///     Ok(0) => {
 1223|       |///         // Child process
 1224|       |///         println!("In child process");
 1225|       |///     },
 1226|       |///     Ok(child_pid) => {
 1227|       |///         // Parent process
 1228|       |///         println!("Forked child with PID: {}", child_pid);
 1229|       |///     },
 1230|       |///     Err(e) => {
 1231|       |///         eprintln!("Fork failed: {}", e);
 1232|       |///     }
 1233|       |/// }
 1234|       |/// ```
 1235|      0|pub fn gc_safe_fork() -> Result<libc::pid_t, std::io::Error> {
 1236|       |    use crate::memory::COLLECTOR;
 1237|       |
 1238|      0|    COLLECTOR.suspend_for_fork();
 1239|       |
 1240|      0|    let result = unsafe { libc::fork() };
 1241|       |
 1242|      0|    COLLECTOR.resume_after_fork();
 1243|       |
 1244|      0|    if result == -1 {
 1245|      0|        Err(std::io::Error::last_os_error())
 1246|       |    } else {
 1247|      0|        Ok(result)
 1248|       |    }
 1249|      0|}

/home/martin/git/fugrip/src/gc_ref_wrappers.rs:
    1|       |// High-level GC reference wrappers
    2|       |// This module provides safe wrappers for accessing garbage-collected objects
    3|       |
    4|       |// AtomicU8 used via fully-qualified path later; no direct import needed
    5|       |
    6|       |// Import core types and traits
    7|       |use crate::traits::GcTrace;
    8|       |use crate::types::{Gc, Weak};
    9|       |
   10|       |// WeakRef is now defined in types.rs to avoid duplication
   11|       |
   12|       |impl<T: GcTrace + 'static> crate::types::WeakRef<T> {
   13|       |    /// Attempts to upgrade this weak reference to a strong reference.
   14|       |    /// Returns `Some(Gc<T>)` if the target object is still alive, `None` otherwise.
   15|      0|    pub fn upgrade(&self) -> Option<Gc<T>> {
   16|      0|        unsafe { (*self.weak).upgrade() }
   17|      0|    }
   18|       |}
   19|       |
   20|       |impl<T: GcTrace + 'static> Weak<T> {
   21|       |    /// Thin wrapper that forwards to the existing `upgrade` implementation.
   22|      2|    pub fn try_upgrade(&self) -> Option<Gc<T>> {
   23|      2|        self.upgrade()
   24|      2|    }
   25|       |}
   26|       |
   27|       |/// Trait for types that can be finalized before garbage collection.
   28|       |pub trait Finalizable {
   29|       |    fn finalize(&mut self);
   30|       |}
   31|       |
   32|       |/// A finalizable wrapper used for tests and examples.
   33|       |pub struct FinalizableObject<T: Finalizable> {
   34|       |    pub data: T,
   35|       |    pub finalize_state: std::sync::atomic::AtomicU8,
   36|       |}
   37|       |
   38|       |impl<T: Finalizable> FinalizableObject<T> {
   39|      6|    pub fn new(data: T) -> Self {
   40|      6|        FinalizableObject {
   41|      6|            data,
   42|      6|            finalize_state: std::sync::atomic::AtomicU8::new(0),
   43|      6|        }
   44|      6|    }
   45|       |
   46|      3|    pub fn mark_finalized(&self) {
   47|      3|        self.finalize_state.store(1, std::sync::atomic::Ordering::Release);
   48|      3|    }
   49|       |}
   50|       |
   51|       |/// Guard types for read/write access control
   52|       |pub struct ReadGuard;
   53|       |pub struct WriteGuard;
   54|       |
   55|       |/// Macro to easily implement GcTrace for types
   56|       |#[macro_export]
   57|       |macro_rules! gc_traceable {
   58|       |    ($t:ty) => {
   59|       |        unsafe impl $crate::traits::GcTrace for $t {
   60|      4|            unsafe fn trace(&self, _stack: &mut Vec<$crate::SendPtr<$crate::GcHeader<()>>>) {
   61|       |                // Default implementation for types without GC references
   62|      4|            }
   63|       |        }
   64|       |    };
   65|       |}

/home/martin/git/fugrip/src/interfaces.rs:
    1|       |use crate::traits::GcTrace;
    2|       |use crate::types::{
    3|       |    CollectorPhase, Gc, GcConfig, GcHeader, GcResult, GcStats, ObjectClass, SendPtr,
    4|       |};
    5|       |
    6|       |/// Interface for garbage-collected memory allocation.
    7|       |///
    8|       |/// This trait defines the contract for allocators that can create
    9|       |/// garbage-collected objects with different classifications.
   10|       |///
   11|       |/// # Examples
   12|       |///
   13|       |/// ```
   14|       |/// use fugrip::{AllocatorTrait, ObjectClass};
   15|       |/// # use fugrip::traits::GcTrace;
   16|       |/// # use fugrip::{Gc, SendPtr, GcHeader};
   17|       |/// # struct TestData(i32);
   18|       |/// # unsafe impl GcTrace for TestData {
   19|       |/// #     unsafe fn trace(&self, _: &mut Vec<SendPtr<GcHeader<()>>>) {}
   20|       |/// # }
   21|       |///
   22|       |/// // Example implementation sketch (actual implementation would be much more complex)
   23|       |/// struct MockAllocator;
   24|       |/// # impl AllocatorTrait for MockAllocator {
   25|       |/// #     fn allocate_classified<T: GcTrace + 'static>(&self, value: T, _class: ObjectClass) -> Gc<T> {
   26|       |/// #         // This is a mock - real implementation would allocate in GC heap
   27|       |/// #         unimplemented!("This is just a mock for demonstration")
   28|       |/// #     }
   29|       |/// #     fn bytes_allocated(&self) -> usize { 0 }
   30|       |/// #     fn object_count(&self) -> usize { 0 }
   31|       |/// # }
   32|       |///
   33|       |/// let allocator = MockAllocator;
   34|       |/// 
   35|       |/// // Allocate with default classification
   36|       |/// // let gc_obj = allocator.allocate(TestData(42));
   37|       |/// 
   38|       |/// // Allocate with specific classification  
   39|       |/// // let gc_finalizable = allocator.allocate_classified(TestData(99), ObjectClass::Finalizer);
   40|       |/// 
   41|       |/// // Get allocation statistics
   42|       |/// let bytes_used = allocator.bytes_allocated();
   43|       |/// let object_count = allocator.object_count();
   44|       |/// assert_eq!(bytes_used, 0); // Mock allocator returns 0
   45|       |/// assert_eq!(object_count, 0);
   46|       |/// ```
   47|       |pub trait AllocatorTrait {
   48|       |    /// Allocates a garbage-collected object with the specified classification.
   49|       |    fn allocate_classified<T: GcTrace + 'static>(&self, value: T, class: ObjectClass) -> Gc<T>;
   50|       |
   51|       |    /// Allocates a standard garbage-collected object.
   52|      1|    fn allocate<T: GcTrace + 'static>(&self, value: T) -> Gc<T> {
   53|      1|        self.allocate_classified(value, ObjectClass::Default)
   54|      1|    }
   55|       |
   56|       |    /// Gets the total number of bytes allocated by this allocator.
   57|       |    fn bytes_allocated(&self) -> usize;
   58|       |
   59|       |    /// Gets the number of objects currently allocated.
   60|       |    fn object_count(&self) -> usize;
   61|       |}
   62|       |
   63|       |/// Interface for heap management operations.
   64|       |///
   65|       |/// This trait provides methods for managing heap segments and
   66|       |/// controlling heap growth and shrinkage.
   67|       |pub trait HeapManager {
   68|       |    /// Adds a new segment to the heap.
   69|       |    fn add_segment(&self, size: usize) -> GcResult<usize>;
   70|       |
   71|       |    /// Removes an empty segment from the heap.
   72|       |    fn remove_segment(&self, segment_id: usize) -> GcResult<()>;
   73|       |
   74|       |    /// Gets the current number of segments in the heap.
   75|       |    fn segment_count(&self) -> usize;
   76|       |
   77|       |    /// Gets the total heap size in bytes.
   78|       |    fn total_heap_size(&self) -> usize;
   79|       |
   80|       |    /// Gets the amount of free space in the heap.
   81|       |    fn free_space(&self) -> usize;
   82|       |
   83|       |    /// Triggers heap compaction if supported.
   84|       |    fn compact(&self) -> GcResult<()>;
   85|       |}
   86|       |
   87|       |/// Interface for garbage collection operations.
   88|       |///
   89|       |/// This trait defines the contract for garbage collectors that can
   90|       |/// perform different types of collection cycles.
   91|       |///
   92|       |/// # Examples
   93|       |///
   94|       |/// ```
   95|       |/// use fugrip::{GarbageCollector, CollectorPhase, GcStats, GcResult};
   96|       |///
   97|       |/// // Mock garbage collector for demonstration
   98|       |/// struct MockGC {
   99|       |///     phase: CollectorPhase,
  100|       |///     collecting: bool,
  101|       |/// }
  102|       |///
  103|       |/// # impl MockGC {
  104|       |/// #     fn new() -> Self { 
  105|       |/// #         MockGC { 
  106|       |/// #             phase: CollectorPhase::Waiting, 
  107|       |/// #             collecting: false 
  108|       |/// #         } 
  109|       |/// #     }
  110|       |/// # }
  111|       |/// # 
  112|       |/// # impl GarbageCollector for MockGC {
  113|       |/// #     fn request_collection(&self) {}
  114|       |/// #     fn collect_full(&self) -> GcResult<GcStats> { 
  115|       |/// #         Ok(GcStats::default()) 
  116|       |/// #     }
  117|       |/// #     fn collect_minor(&self) -> GcResult<GcStats> { 
  118|       |/// #         Ok(GcStats::default()) 
  119|       |/// #     }
  120|       |/// #     fn current_phase(&self) -> CollectorPhase { 
  121|       |/// #         self.phase 
  122|       |/// #     }
  123|       |/// #     fn is_collecting(&self) -> bool { 
  124|       |/// #         self.collecting 
  125|       |/// #     }
  126|       |/// #     fn suspend_collection(&self) {}
  127|       |/// #     fn resume_collection(&self) {}
  128|       |/// #     fn statistics(&self) -> GcStats { 
  129|       |/// #         GcStats::default() 
  130|       |/// #     }
  131|       |/// # }
  132|       |///
  133|       |/// let gc = MockGC::new();
  134|       |/// 
  135|       |/// // Check collector state
  136|       |/// assert_eq!(gc.current_phase(), CollectorPhase::Waiting);
  137|       |/// assert!(!gc.is_collecting());
  138|       |/// 
  139|       |/// // Request collection
  140|       |/// gc.request_collection();
  141|       |/// 
  142|       |/// // Get statistics
  143|       |/// let stats = gc.statistics();
  144|       |/// assert_eq!(stats.total_collections, 0); // Mock returns defaults
  145|       |/// ```
  146|       |pub trait GarbageCollector {
  147|       |    /// Requests a garbage collection cycle.
  148|       |    fn request_collection(&self);
  149|       |
  150|       |    /// Performs a full garbage collection cycle.
  151|       |    fn collect_full(&self) -> GcResult<GcStats>;
  152|       |
  153|       |    /// Performs a minor (young generation) collection.
  154|       |    fn collect_minor(&self) -> GcResult<GcStats>;
  155|       |
  156|       |    /// Gets the current collection phase.
  157|       |    fn current_phase(&self) -> CollectorPhase;
  158|       |
  159|       |    /// Checks if a collection is currently in progress.
  160|       |    fn is_collecting(&self) -> bool;
  161|       |
  162|       |    /// Suspends collection for critical operations.
  163|       |    fn suspend_collection(&self);
  164|       |
  165|       |    /// Resumes collection after suspension.
  166|       |    fn resume_collection(&self);
  167|       |
  168|       |    /// Gets collection statistics.
  169|       |    fn statistics(&self) -> GcStats;
  170|       |}
  171|       |
  172|       |/// Interface for weak reference management.
  173|       |///
  174|       |/// This trait provides methods for creating and managing weak references
  175|       |/// that don't prevent garbage collection of their targets.
  176|       |pub trait WeakReferenceManager {
  177|       |    /// Creates a weak reference to a target object.
  178|       |    fn create_weak<T: GcTrace + 'static>(&self, target: &Gc<T>) -> Gc<crate::types::Weak<T>>;
  179|       |
  180|       |    /// Invalidates all weak references to dead objects.
  181|       |    fn invalidate_dead_references(&self);
  182|       |
  183|       |    /// Gets the count of active weak references.
  184|       |    fn weak_reference_count(&self) -> usize;
  185|       |
  186|       |    /// Performs census operations on weak references.
  187|       |    fn census_weak_references(&self);
  188|       |}
  189|       |
  190|       |/// Interface for object finalization.
  191|       |///
  192|       |/// This trait manages objects that need special handling before
  193|       |/// being garbage collected.
  194|       |pub trait FinalizationManager {
  195|       |    /// Registers an object for finalization.
  196|       |    fn register_for_finalization<T>(&self, obj: &Gc<T>) -> GcResult<()>;
  197|       |
  198|       |    /// Unregisters an object from finalization.
  199|       |    fn unregister_from_finalization<T>(&self, obj: &Gc<T>) -> GcResult<()>;
  200|       |
  201|       |    /// Runs pending finalizers.
  202|       |    fn run_finalizers(&self) -> usize;
  203|       |
  204|       |    /// Gets the count of objects awaiting finalization.
  205|       |    fn finalization_queue_size(&self) -> usize;
  206|       |
  207|       |    /// Checks if an object is registered for finalization.
  208|       |    fn is_registered_for_finalization<T>(&self, obj: &Gc<T>) -> bool;
  209|       |}
  210|       |
  211|       |/// Interface for thread-local allocation buffers.
  212|       |///
  213|       |/// This trait manages fast-path allocation buffers that reduce
  214|       |/// contention during object allocation.
  215|       |pub trait AllocationBuffer {
  216|       |    /// Tries to allocate from the local buffer.
  217|       |    fn try_allocate<T>(&mut self) -> Option<*mut GcHeader<T>>;
  218|       |
  219|       |    /// Refills the buffer from the global heap.
  220|       |    fn refill_buffer(&mut self, size: usize) -> GcResult<()>;
  221|       |
  222|       |    /// Gets the remaining space in the buffer.
  223|       |    fn remaining_space(&self) -> usize;
  224|       |
  225|       |    /// Sets the allocation color for new objects.
  226|       |    fn set_allocating_black(&mut self, black: bool);
  227|       |
  228|       |    /// Checks if currently allocating black objects.
  229|       |    fn is_allocating_black(&self) -> bool;
  230|       |}
  231|       |
  232|       |/// Interface for object iteration during collection phases.
  233|       |///
  234|       |/// This trait provides methods for efficiently iterating over
  235|       |/// objects of specific types during garbage collection.
  236|       |pub trait ObjectIterator {
  237|       |    /// Iterates over objects in parallel using multiple worker threads.
  238|       |    fn iterate_parallel<F>(&self, worker_count: usize, func: F)
  239|       |    where
  240|       |        F: Fn(SendPtr<GcHeader<()>>) + Send + Sync + 'static;
  241|       |
  242|       |    /// Iterates over objects sequentially.
  243|       |    fn iterate_sequential<F>(&self, func: F)
  244|       |    where
  245|       |        F: Fn(SendPtr<GcHeader<()>>);
  246|       |
  247|       |    /// Gets the count of objects managed by this iterator.
  248|       |    fn object_count(&self) -> usize;
  249|       |
  250|       |    /// Registers a new object for iteration.
  251|       |    fn register_object(&self, ptr: *mut GcHeader<()>);
  252|       |
  253|       |    /// Removes an object from iteration.
  254|       |    fn unregister_object(&self, ptr: *mut GcHeader<()>);
  255|       |}
  256|       |
  257|       |/// Interface for type information management.
  258|       |///
  259|       |/// This trait manages runtime type information for garbage-collected
  260|       |/// objects, enabling proper tracing and cleanup.
  261|       |pub trait TypeInfoManager {
  262|       |    /// Gets type information for a specific type.
  263|       |    fn get_type_info<T: GcTrace + 'static>(&self) -> &'static crate::types::TypeInfo;
  264|       |
  265|       |    /// Registers type information for a new type.
  266|       |    fn register_type<T: GcTrace + 'static>(&self) -> &'static crate::types::TypeInfo;
  267|       |
  268|       |    /// Gets the count of registered types.
  269|       |    fn registered_type_count(&self) -> usize;
  270|       |
  271|       |    /// Clears cached type information.
  272|       |    fn clear_cache(&self);
  273|       |}
  274|       |
  275|       |/// Interface for conservative pointer scanning.
  276|       |///
  277|       |/// This trait provides methods for scanning memory regions to find
  278|       |/// potential pointers to garbage-collected objects.
  279|       |pub trait ConservativeScanner {
  280|       |    /// Scans a memory region for potential object pointers.
  281|       |    /// # Safety
  282|       |    /// The caller must ensure `start..end` is a valid readable memory range
  283|       |    /// and that concurrent modifications will not invalidate pointers found
  284|       |    /// within that region. The implementation may dereference raw pointers.
  285|       |    unsafe fn scan_region(
  286|       |        &self,
  287|       |        start: *const u8,
  288|       |        end: *const u8,
  289|       |        roots: &mut Vec<*mut GcHeader<()>>,
  290|       |    );
  291|       |
  292|       |    /// Scans the current thread's stack for roots.
  293|       |    fn scan_stack(&self, roots: &mut Vec<*mut GcHeader<()>>);
  294|       |
  295|       |    /// Scans static/global memory regions for roots.
  296|       |    fn scan_globals(&self, roots: &mut Vec<*mut GcHeader<()>>);
  297|       |
  298|       |    /// Validates that a pointer points to a valid object.
  299|       |    /// # Safety
  300|       |    /// The pointer may be unaligned or dangling; the implementation may
  301|       |    /// dereference it. The caller must ensure this call is safe to perform
  302|       |    /// in the current collection context.
  303|       |    unsafe fn validate_pointer(&self, ptr: *const u8) -> bool;
  304|       |}
  305|       |
  306|       |/// Interface for collection phase coordination.
  307|       |///
  308|       |/// This trait manages the state machine for garbage collection phases
  309|       |/// and coordinates between mutator threads and collector threads.
  310|       |pub trait PhaseCoordinator {
  311|       |    /// Transitions to a new collection phase.
  312|       |    fn transition_to_phase(&self, phase: CollectorPhase) -> GcResult<()>;
  313|       |
  314|       |    /// Waits for all threads to reach a synchronization point.
  315|       |    fn synchronize_threads(&self) -> GcResult<()>;
  316|       |
  317|       |    /// Signals that a thread has reached a safe point.
  318|       |    fn signal_safe_point(&self);
  319|       |
  320|       |    /// Checks if it's safe to perform collection operations.
  321|       |    fn is_safe_to_collect(&self) -> bool;
  322|       |
  323|       |    /// Gets the current phase.
  324|       |    fn current_phase(&self) -> CollectorPhase;
  325|       |
  326|       |    /// Requests a phase transition.
  327|       |    fn request_phase_transition(&self, phase: CollectorPhase);
  328|       |}
  329|       |
  330|       |/// Interface for fork safety operations.
  331|       |///
  332|       |/// This trait provides methods for safely handling process forking
  333|       |/// in the presence of garbage collection.
  334|       |pub trait ForkSafetyManager {
  335|       |    /// Suspends garbage collection before forking.
  336|       |    fn suspend_for_fork(&self);
  337|       |
  338|       |    /// Resumes garbage collection after forking in parent process.
  339|       |    fn resume_after_fork_parent(&self);
  340|       |
  341|       |    /// Initializes garbage collection in child process after forking.
  342|       |    fn initialize_after_fork_child(&self);
  343|       |
  344|       |    /// Checks if the collector is currently suspended for forking.
  345|       |    fn is_suspended_for_fork(&self) -> bool;
  346|       |}
  347|       |
  348|       |/// Interface for GC configuration and tuning.
  349|       |///
  350|       |/// This trait provides methods for configuring garbage collector
  351|       |/// behavior and performance parameters.
  352|       |pub trait ConfigurationManager {
  353|       |    /// Gets the current configuration.
  354|       |    fn get_config(&self) -> &GcConfig;
  355|       |
  356|       |    /// Updates the configuration.
  357|       |    fn set_config(&self, config: GcConfig) -> GcResult<()>;
  358|       |
  359|       |    /// Gets a specific configuration parameter.
  360|       |    fn get_parameter<T>(&self, name: &str) -> Option<T>
  361|       |    where
  362|       |        T: Clone + 'static;
  363|       |
  364|       |    /// Sets a specific configuration parameter.
  365|       |    fn set_parameter<T>(&self, name: &str, value: T) -> GcResult<()>
  366|       |    where
  367|       |        T: Clone + 'static;
  368|       |
  369|       |    /// Resets configuration to default values.
  370|       |    fn reset_to_defaults(&self);
  371|       |}
  372|       |
  373|       |/// Interface for debugging and profiling support.
  374|       |///
  375|       |/// This trait provides methods for debugging garbage collection
  376|       |/// behavior and profiling performance characteristics.
  377|       |pub trait DebugManager {
  378|       |    /// Enables debug logging for specific components.
  379|       |    fn enable_debug_logging(&self, components: &[&str]);
  380|       |
  381|       |    /// Disables debug logging.
  382|       |    fn disable_debug_logging(&self);
  383|       |
  384|       |    /// Dumps heap state for debugging.
  385|       |    fn dump_heap_state(&self) -> String;
  386|       |
  387|       |    /// Validates heap integrity.
  388|       |    fn validate_heap(&self) -> GcResult<()>;
  389|       |
  390|       |    /// Starts profiling collection operations.
  391|       |    fn start_profiling(&self);
  392|       |
  393|       |    /// Stops profiling and returns results.
  394|       |    fn stop_profiling(&self) -> std::collections::HashMap<String, u64>;
  395|       |
  396|       |    /// Forces a specific type of collection for testing.
  397|       |    fn force_collection(&self, phase: CollectorPhase) -> GcResult<()>;
  398|       |}

/home/martin/git/fugrip/src/lib.rs:
    1|       |// Core types and contracts (no dependencies)
    2|       |pub mod interfaces;
    3|       |pub mod traits;
    4|       |pub mod types;
    5|       |
    6|       |// Implementation modules (depend on core types)
    7|       |pub mod collector_phases;
    8|       |pub mod gc_ref_wrappers;
    9|       |pub mod memory;
   10|       |
   11|       |// Re-export core types
   12|       |pub use types::*;
   13|       |// Re-export selected traits to avoid ambiguous glob re-exports
   14|       |pub use traits::GcTrace;
   15|       |// Re-export interfaces (AllocatorTrait, etc.)
   16|       |pub use gc_ref_wrappers::Finalizable;
   17|       |pub use interfaces::{
   18|       |    AllocatorTrait, GarbageCollector, HeapManager, ObjectIterator,
   19|       |};
   20|       |pub use memory::{CLASSIFIED_ALLOCATOR, ClassifiedAllocator, ObjectSet, register_root, scan_stacks, ROOTS};
   21|       |pub use types::{
   22|       |    CollectorPhase, FREE_SINGLETON_TYPE_INFO, FreeSingleton, Gc, GcError, GcHeader, GcRef,
   23|       |    GcRefMut, GcResult, ObjectClass, ObjectType, SendPtr, ThreadLocalBuffer, TypeInfo, align_up,
   24|       |};
   25|       |
   26|       |#[cfg(test)]
   27|       |mod tests {
   28|       |    use super::*;
   29|       |
   30|       |    #[test]
   31|      1|    fn test_basic_gc_allocation() {
   32|       |        // Test that we can allocate a simple value
   33|      1|        let gc_int = Gc::new(42i32);
   34|       |
   35|       |        // Test that we can read the value
   36|      1|        let value = gc_int.read().unwrap();
   37|      1|        assert_eq!(*value, 42);
   38|      1|    }
   39|       |
   40|       |    #[test]
   41|      1|    fn test_gc_string_allocation() {
   42|       |        // Test with a more complex type
   43|      1|        let gc_string = Gc::new("Hello, FUGC!".to_string());
   44|       |
   45|      1|        let value = gc_string.read().unwrap();
   46|      1|        assert_eq!(*value, "Hello, FUGC!");
   47|      1|    }
   48|       |
   49|       |    #[test]
   50|      1|    fn test_multiple_allocations() {
   51|       |        // Test that we can allocate many objects
   52|      1|        let mut gcs = Vec::new();
   53|    101|        for i in 0..100 {
                          ^100
   54|    100|            gcs.push(Gc::new(i));
   55|    100|        }
   56|       |
   57|       |        // Verify all values are correct
   58|    100|        for (i, gc) in gcs.iter().enumerate() {
                                     ^1         ^1
   59|    100|            let value = gc.read().unwrap();
   60|    100|            assert_eq!(*value, i);
   61|       |        }
   62|      1|    }
   63|       |
   64|       |    #[test]
   65|      1|    fn test_gc_write() {
   66|      1|        let gc_int = Gc::new(0i32);
   67|       |
   68|       |        // Test mutable access (should work when not marking)
   69|      1|        if let Some(mut value) = gc_int.write() {
   70|      1|            *value = 99;
   71|      1|        }
                      ^0
   72|       |
   73|      1|        let value = gc_int.read().unwrap();
   74|      1|        assert_eq!(*value, 99);
   75|      1|    }
   76|       |
   77|       |    #[test]
   78|      1|    fn test_weak_reference_creation() {
   79|       |        // Create a strong reference
   80|      1|        let strong_ref = Gc::new(42i32);
   81|       |
   82|       |        // Create a weak reference to it
   83|      1|        let weak_ref = Weak::new_simple(&strong_ref);
   84|       |
   85|       |        // Test that upgrade works
   86|      1|        if let Some(upgraded) = weak_ref.read().unwrap().upgrade() {
   87|      1|            let value = upgraded.read().unwrap();
   88|      1|            assert_eq!(*value, 42);
   89|       |        } else {
   90|      0|            panic!("Weak reference upgrade failed");
   91|       |        }
   92|      1|    }
   93|       |
   94|       |    #[test]
   95|      1|    fn test_census_phase() {
   96|       |        use crate::collector_phases::CollectorState;
   97|       |        use crate::memory::CLASSIFIED_ALLOCATOR;
   98|       |
   99|       |        // Create some weak references
  100|      1|        let strong_ref1 = Gc::new(1i32);
  101|      1|        let strong_ref2 = Gc::new(2i32);
  102|      1|        let _weak_ref1 = Weak::new_simple(&strong_ref1);
  103|      1|        let _weak_ref2 = Weak::new_simple(&strong_ref2);
  104|       |
  105|       |        // Verify we have some weak references registered
  106|      1|        let weak_set = CLASSIFIED_ALLOCATOR.get_weak_object_set();
  107|      1|        assert!(weak_set.get_object_count() >= 2);
  108|       |
  109|       |        // Create a collector and run census phase (should work without panic)
  110|      1|        let collector = CollectorState::new();
  111|      1|        collector.census_phase();
  112|       |
  113|       |        // Census phase completed successfully
  114|      1|        assert!(true);
  115|      1|    }
  116|       |
  117|       |    #[test]
  118|      1|    fn test_type_info_caching() {
  119|       |        use crate::types::type_info;
  120|       |
  121|       |        // Get TypeInfo for the same type multiple times
  122|      1|        let type_info1 = type_info::<i32>();
  123|      1|        let type_info2 = type_info::<i32>();
  124|      1|        let type_info3 = type_info::<String>();
  125|      1|        let type_info4 = type_info::<String>();
  126|       |
  127|       |        // Verify that the same type returns the same TypeInfo instance (cached)
  128|      1|        assert!(
  129|      1|            std::ptr::eq(type_info1, type_info2),
  130|      0|            "i32 TypeInfo should be cached"
  131|       |        );
  132|      1|        assert!(
  133|      1|            std::ptr::eq(type_info3, type_info4),
  134|      0|            "String TypeInfo should be cached"
  135|       |        );
  136|       |
  137|       |        // Verify that different types return different TypeInfo instances
  138|      1|        assert!(
  139|      1|            !std::ptr::eq(type_info1, type_info3),
  140|      0|            "Different types should have different TypeInfo"
  141|       |        );
  142|       |
  143|       |        // Verify basic properties
  144|      1|        assert_eq!(type_info1.size, std::mem::size_of::<GcHeader<i32>>());
  145|      1|        assert_eq!(type_info3.size, std::mem::size_of::<GcHeader<String>>());
  146|      1|    }
  147|       |
  148|       |    #[test]
  149|      1|    fn test_forwarding_pointer_chain() {
  150|       |        use std::sync::atomic::Ordering;
  151|       |
  152|       |        // Create a GC pointer
  153|      1|        let gc1 = Gc::new(42i32);
  154|      1|        let gc2 = Gc::new(99i32);
  155|       |
  156|       |        // Simulate a forwarding pointer from gc1 to gc2
  157|       |        // This would normally happen during garbage collection
  158|      1|        unsafe {
  159|      1|            (*gc1.as_ptr())
  160|      1|                .forwarding_ptr
  161|      1|                .store(gc2.as_ptr() as *mut GcHeader<()>, Ordering::Release);
  162|      1|        }
  163|       |
  164|       |        // Reading from gc1 should follow the forwarding pointer and return gc2's value
  165|      1|        let value = gc1.read().unwrap();
  166|      1|        assert_eq!(*value, 99); // Should read the forwarded value
  167|       |
  168|       |        // After following the forwarding pointer, gc1 should be updated to point to gc2
  169|       |        // This optimization ensures future accesses are fast
  170|      1|        assert_eq!(gc1.as_ptr(), gc2.as_ptr()); // gc1 should now point to gc2
  171|      1|    }
  172|       |}

/home/martin/git/fugrip/src/memory.rs:
    1|       |use crate::collector_phases::{CollectorState, MUTATOR_STATE};
    2|       |use crate::interfaces::*;
    3|       |use crate::traits::*;
    4|       |use crate::types::*;
    5|       |use once_cell::sync::Lazy;
    6|       |use parking_lot;
    7|       |use std::marker::PhantomData;
    8|       |use std::mem::MaybeUninit;
    9|       |use std::sync::Arc;
   10|       |use std::sync::Mutex;
   11|       |use std::sync::atomic::{AtomicBool, AtomicPtr, AtomicUsize, Ordering};
   12|       |
   13|       |/// High-level garbage-collected allocator.
   14|       |///
   15|       |/// `GcAllocator` provides the main allocation interface for the garbage collector.
   16|       |/// It manages both thread-local fast-path allocation and global slow-path allocation
   17|       |/// with automatic garbage collection triggering based on allocation thresholds.
   18|       |///
   19|       |/// # Examples
   20|       |///
   21|       |/// ```
   22|       |/// use fugrip::memory::GcAllocator;
   23|       |///
   24|       |/// let allocator = GcAllocator::new();
   25|       |/// // Allocation happens automatically through Gc::new()
   26|       |/// ```
   27|       |pub struct GcAllocator {
   28|       |    heap: SegmentedHeap,
   29|       |    collector: Arc<CollectorState>,
   30|       |    allocation_threshold: AtomicUsize,
   31|       |    live_bytes: AtomicUsize,
   32|       |}
   33|       |
   34|       |impl Default for GcAllocator {
   35|      0|    fn default() -> Self {
   36|      0|        Self::new()
   37|      0|    }
   38|       |}
   39|       |
   40|       |impl GcAllocator {
   41|       |    /// Creates a new garbage collector allocator with default settings.
   42|       |    ///
   43|       |    /// The allocator starts with a 1MB allocation threshold that triggers
   44|       |    /// garbage collection when exceeded.
   45|       |    ///
   46|       |    /// # Examples
   47|       |    ///
   48|       |    /// ```
   49|       |    /// use fugrip::memory::GcAllocator;
   50|       |    ///
   51|       |    /// let allocator = GcAllocator::new();
   52|       |    /// ```
   53|      4|    pub fn new() -> Self {
   54|      4|        GcAllocator {
   55|      4|            heap: SegmentedHeap::new(),
   56|      4|            collector: COLLECTOR.clone(),
   57|      4|            allocation_threshold: AtomicUsize::new(1024 * 1024), // 1MB initial threshold
   58|      4|            live_bytes: AtomicUsize::new(0),
   59|      4|        }
   60|      4|    }
   61|       |
   62|       |    /// Allocates a garbage-collected object of type `T`.
   63|       |    ///
   64|       |    /// This method uses a two-tier allocation strategy:
   65|       |    /// 1. Fast path: Try thread-local allocation from allocation buffers
   66|       |    /// 2. Slow path: Fall back to global heap allocation with GC triggering
   67|       |    ///
   68|       |    /// # Parameters
   69|       |    ///
   70|       |    /// * `value` - The value to store in the garbage-collected heap
   71|       |    ///
   72|       |    /// # Returns
   73|       |    ///
   74|       |    /// A `Gc<T>` pointer to the allocated object
   75|       |    ///
   76|       |    /// # Examples
   77|       |    ///
   78|       |    /// ```
   79|       |    /// use fugrip::{Gc, memory::GcAllocator};
   80|       |    ///
   81|       |    /// let allocator = GcAllocator::new();
   82|       |    /// let gc_value = allocator.allocate_gc(42i32);
   83|       |    /// // Normally called automatically via Gc::new()
   84|       |    /// ```
   85|  52.5k|    pub fn allocate_gc<T: GcTrace + 'static>(&self, value: T) -> Gc<T> {
   86|       |        // Fast path: thread-local allocation
   87|  52.5k|        let ptr = MUTATOR_STATE.with(|state| state.borrow_mut().try_allocate::<T>());
   88|       |
   89|  52.5k|        if let Some(ptr) = ptr {
                                  ^0
   90|      0|            let allocating_black = MUTATOR_STATE.with(|state| state.borrow().allocating_black);
   91|       |            unsafe {
   92|      0|                let header = GcHeader {
   93|      0|                    mark_bit: AtomicBool::new(allocating_black),
   94|      0|                    type_info: type_info::<T>(),
   95|      0|                    forwarding_ptr: AtomicPtr::new(std::ptr::null_mut()),
   96|      0|                    weak_ref_list: AtomicPtr::new(std::ptr::null_mut()),
   97|      0|                    data: value,
   98|      0|                };
   99|      0|                std::ptr::write(ptr, header);
  100|      0|                return Gc {
  101|      0|                    ptr: AtomicPtr::new(ptr),
  102|      0|                    _phantom: PhantomData,
  103|      0|                };
  104|       |            }
  105|  52.5k|        }
  106|       |
  107|       |        // Slow path: global allocation with potential GC trigger
  108|  52.5k|        self.allocate_slow_path(value)
  109|  52.5k|    }
  110|       |
  111|  52.5k|    fn allocate_slow_path<T: GcTrace + 'static>(&self, value: T) -> Gc<T> {
  112|       |        // Check if we need to trigger GC
  113|  52.5k|        if self.live_bytes.load(Ordering::Relaxed)
  114|  52.5k|            > self.allocation_threshold.load(Ordering::Relaxed)
  115|      0|        {
  116|      0|            self.collector.request_collection();
  117|  52.5k|        }
  118|       |
  119|       |        // Delegate to the heap for actual allocation
  120|  52.5k|        self.heap.allocate(value)
  121|  52.5k|    }
  122|       |
  123|       |    /// Get access to the underlying segmented heap for GC operations.
  124|       |    /// This is used by the collector during sweeping phases.
  125|      0|    pub fn get_heap(&self) -> &SegmentedHeap {
  126|      0|        &self.heap
  127|      0|    }
  128|       |}
  129|       |
  130|       |/// Segmented allocator that manages garbage-collected memory.
  131|       |///
  132|       |/// The `SegmentedHeap` divides memory into large segments to reduce contention
  133|       |/// and improve allocation performance. It uses lock-free allocation within
  134|       |/// segments and falls back to segment expansion when needed.
  135|       |///
  136|       |/// # Examples
  137|       |///
  138|       |/// ```
  139|       |/// use fugrip::memory::SegmentedHeap;
  140|       |///
  141|       |/// let heap = SegmentedHeap::new();
  142|       |/// // Allocation happens automatically through Gc::new()
  143|       |/// ```
  144|       |pub struct SegmentedHeap {
  145|       |    pub segments: Mutex<Vec<Segment>>,
  146|       |    pub current_segment: AtomicUsize,
  147|       |    pub collector_state: Arc<CollectorState>,
  148|       |}
  149|       |
  150|       |impl Default for SegmentedHeap {
  151|      0|    fn default() -> Self {
  152|      0|        Self::new()
  153|      0|    }
  154|       |}
  155|       |
  156|       |impl SegmentedHeap {
  157|       |    /// Creates a new segmented heap with an initial segment.
  158|       |    ///
  159|       |    /// # Examples
  160|       |    ///
  161|       |    /// ```
  162|       |    /// use fugrip::memory::SegmentedHeap;
  163|       |    ///
  164|       |    /// let heap = SegmentedHeap::new();
  165|       |    /// assert_eq!(heap.segment_count(), 1);
  166|       |    /// ```
  167|     29|    pub fn new() -> Self {
  168|     29|        let initial_segment = Segment::new(0);
  169|     29|        SegmentedHeap {
  170|     29|            segments: Mutex::new(vec![initial_segment]),
  171|     29|            current_segment: AtomicUsize::new(0),
  172|     29|            collector_state: COLLECTOR.clone(),
  173|     29|        }
  174|     29|    }
  175|       |
  176|       |    /// Adds a new segment to the heap and returns its ID.
  177|       |    ///
  178|       |    /// This method is called automatically when existing segments are full.
  179|       |    /// Each segment provides 1MB of memory for allocation.
  180|       |    ///
  181|       |    /// # Returns
  182|       |    ///
  183|       |    /// The ID of the newly created segment.
  184|       |    ///
  185|       |    /// # Examples
  186|       |    ///
  187|       |    /// ```
  188|       |    /// use fugrip::memory::SegmentedHeap;
  189|       |    ///
  190|       |    /// let heap = SegmentedHeap::new();
  191|       |    /// let initial_count = heap.segment_count();
  192|       |    /// let new_id = heap.add_segment();
  193|       |    /// assert_eq!(heap.segment_count(), initial_count + 1);
  194|       |    /// assert_eq!(new_id, initial_count);
  195|       |    /// ```
  196|      4|    pub fn add_segment(&self) -> usize {
  197|      4|        let mut segments = self.segments.lock().unwrap();
  198|      4|        let new_id = segments.len();
  199|      4|        let new_segment = Segment::new(new_id);
  200|      4|        segments.push(new_segment);
  201|      4|        new_id
  202|      4|    }
  203|       |
  204|       |    /// Returns the current number of segments in the heap.
  205|       |    ///
  206|       |    /// # Examples
  207|       |    ///
  208|       |    /// ```
  209|       |    /// use fugrip::memory::SegmentedHeap;
  210|       |    ///
  211|       |    /// let heap = SegmentedHeap::new();
  212|       |    /// assert_eq!(heap.segment_count(), 1); // Initial segment
  213|       |    ///
  214|       |    /// heap.add_segment();
  215|       |    /// assert_eq!(heap.segment_count(), 2);
  216|       |    /// ```
  217|      5|    pub fn segment_count(&self) -> usize {
  218|      5|        self.segments.lock().unwrap().len()
  219|      5|    }
  220|       |
  221|  52.5k|    pub fn allocate<T: GcTrace + 'static>(&self, value: T) -> Gc<T> {
  222|  52.5k|        let size = std::mem::size_of::<GcHeader<T>>();
  223|  52.5k|        let align = std::mem::align_of::<GcHeader<T>>();
  224|       |
  225|       |        // Try current segment first
  226|  52.5k|        let current_seg_idx = self
  227|  52.5k|            .current_segment
  228|  52.5k|            .load(std::sync::atomic::Ordering::Relaxed);
  229|  52.5k|        if let Some(ptr) = self.try_allocate_raw_in_segment(current_seg_idx, size, align) {
                                  ^52.5k
  230|  52.5k|            return self.initialize_gc_at(ptr, value);
  231|      3|        }
  232|       |
  233|       |        // Try other segments
  234|      3|        let segment_count = self.segment_count();
  235|      6|        for i in 0..segment_count {
                                  ^3
  236|      6|            if i != current_seg_idx
  237|      3|                && let Some(ptr) = self.try_allocate_raw_in_segment(i, size, align)
                                          ^0
  238|       |            {
  239|       |                // Update current segment hint
  240|      0|                self.current_segment
  241|      0|                    .store(i, std::sync::atomic::Ordering::Relaxed);
  242|      0|                return self.initialize_gc_at(ptr, value);
  243|      6|            }
  244|       |        }
  245|       |
  246|       |        // All segments full - add a new segment and try again
  247|      3|        let new_seg_id = self.add_segment();
  248|      3|        if let Some(ptr) = self.try_allocate_raw_in_segment(new_seg_id, size, align) {
  249|      3|            self.current_segment
  250|      3|                .store(new_seg_id, std::sync::atomic::Ordering::Relaxed);
  251|      3|            return self.initialize_gc_at(ptr, value);
  252|      0|        }
  253|       |
  254|       |        // Even new segment failed - this shouldn't happen
  255|      0|        panic!("Failed to allocate even in new segment")
  256|  52.5k|    }
  257|       |
  258|  52.5k|    fn try_allocate_raw_in_segment(
  259|  52.5k|        &self,
  260|  52.5k|        seg_idx: usize,
  261|  52.5k|        size: usize,
  262|  52.5k|        align: usize,
  263|  52.5k|    ) -> Option<*mut u8> {
  264|  52.5k|        let segments = self.segments.lock().unwrap();
  265|  52.5k|        if seg_idx >= segments.len() {
  266|      0|            return None;
  267|  52.5k|        }
  268|       |
  269|  52.5k|        let segment = &segments[seg_idx];
  270|       |
  271|       |        loop {
  272|  52.5k|            let current = segment
  273|  52.5k|                .allocation_ptr
  274|  52.5k|                .load(std::sync::atomic::Ordering::Relaxed);
  275|  52.5k|            let aligned = align_up(current, align) as *mut u8;
  276|  52.5k|            let new_ptr = unsafe { aligned.add(size) };
  277|       |
  278|  52.5k|            if new_ptr > segment.end_ptr.load(std::sync::atomic::Ordering::Relaxed) {
  279|      6|                return None; // Segment full
  280|  52.5k|            }
  281|       |
  282|  52.5k|            if segment
  283|  52.5k|                .allocation_ptr
  284|  52.5k|                .compare_exchange_weak(
  285|  52.5k|                    current,
  286|  52.5k|                    new_ptr,
  287|  52.5k|                    std::sync::atomic::Ordering::Relaxed,
  288|  52.5k|                    std::sync::atomic::Ordering::Relaxed,
  289|  52.5k|                )
  290|  52.5k|                .is_ok()
  291|       |            {
  292|  52.5k|                return Some(aligned);
  293|      0|            }
  294|       |        }
  295|  52.5k|    }
  296|       |
  297|  52.5k|    fn initialize_gc_at<T: GcTrace + 'static>(&self, ptr: *mut u8, value: T) -> Gc<T> {
  298|       |        unsafe {
  299|  52.5k|            let header = GcHeader {
  300|  52.5k|                mark_bit: AtomicBool::new(false),
  301|  52.5k|                type_info: type_info::<T>(),
  302|  52.5k|                forwarding_ptr: AtomicPtr::new(std::ptr::null_mut()),
  303|  52.5k|                weak_ref_list: AtomicPtr::new(std::ptr::null_mut()),
  304|  52.5k|                data: value,
  305|  52.5k|            };
  306|  52.5k|            std::ptr::write(ptr as *mut GcHeader<T>, header);
  307|  52.5k|            Gc {
  308|  52.5k|                ptr: AtomicPtr::new(ptr as *mut GcHeader<T>),
  309|  52.5k|                _phantom: PhantomData,
  310|  52.5k|            }
  311|       |        }
  312|  52.5k|    }
  313|       |}
  314|       |
  315|       |pub struct Segment {
  316|       |    pub memory: Box<[MaybeUninit<u8>]>,
  317|       |    pub mark_bits: Box<[AtomicBool]>,
  318|       |    pub allocation_ptr: AtomicPtr<u8>,
  319|       |    pub end_ptr: AtomicPtr<u8>, // Changed to AtomicPtr for thread safety
  320|       |    pub segment_id: usize,
  321|       |}
  322|       |
  323|       |unsafe impl Send for Segment {}
  324|       |unsafe impl Sync for Segment {}
  325|       |
  326|       |impl Segment {
  327|     33|    pub fn new(id: usize) -> Self {
  328|       |        const SEGMENT_SIZE: usize = 1024 * 1024; // 1MB segments
  329|     33|        let memory = vec![MaybeUninit::uninit(); SEGMENT_SIZE].into_boxed_slice();
  330|       |
  331|       |        // Create mark_bits without using vec! macro (AtomicBool doesn't implement Clone)
  332|     33|        let mark_bits_count = SEGMENT_SIZE / 64;
  333|     33|        let mut mark_bits = Vec::with_capacity(mark_bits_count);
  334|   540k|        for _ in 0..mark_bits_count {
                                  ^33
  335|   540k|            mark_bits.push(AtomicBool::new(false));
  336|   540k|        }
  337|     33|        let mark_bits = mark_bits.into_boxed_slice();
  338|       |
  339|     33|        let start_ptr = memory.as_ptr() as *mut u8;
  340|     33|        let end_ptr = unsafe { start_ptr.add(SEGMENT_SIZE) } as *const u8;
  341|       |
  342|     33|        Segment {
  343|     33|            memory,
  344|     33|            mark_bits,
  345|     33|            allocation_ptr: AtomicPtr::new(start_ptr),
  346|     33|            end_ptr: AtomicPtr::new(end_ptr as *mut u8),
  347|     33|            segment_id: id,
  348|     33|        }
  349|     33|    }
  350|       |}
  351|       |
  352|       |// Global references to allocator and collector
  353|       |pub static ALLOCATOR: Lazy<GcAllocator> = Lazy::new(GcAllocator::new);
  354|       |
  355|      5|pub static COLLECTOR: Lazy<Arc<CollectorState>> = Lazy::new(|| Arc::new(CollectorState::new()));
  356|       |
  357|       |// Safe interface that prevents direct mutation during GC
  358|       |impl<T: GcTrace + 'static> Gc<T> {
  359|       |    /// Allocates a new garbage-collected value.
  360|       |    ///
  361|       |    /// # Examples
  362|       |    ///
  363|       |    /// ```
  364|       |    /// use fugrip::Gc;
  365|       |    ///
  366|       |    /// let gc_value = Gc::new(42i32);
  367|       |    /// ```
  368|  52.5k|    pub fn new(value: T) -> Self {
  369|  52.5k|        ALLOCATOR.allocate_gc(value)
  370|  52.5k|    }
  371|       |
  372|       |    /// Get a read-only reference to the object.
  373|  1.14k|    pub fn read(&self) -> Option<GcRef<'_, T>> {
  374|       |        unsafe {
  375|       |            // Follow forwarding pointers and implement pointer compression
  376|  1.14k|            let mut current_ptr = self.ptr.load(std::sync::atomic::Ordering::Acquire);
  377|  1.14k|            let original_ptr = current_ptr;
  378|       |            
  379|       |            loop {
  380|  1.18k|                let forwarding_ptr = (*current_ptr)
  381|  1.18k|                    .forwarding_ptr
  382|  1.18k|                    .load(std::sync::atomic::Ordering::Acquire);
  383|  1.18k|                if forwarding_ptr.is_null() {
  384|  1.14k|                    break;
  385|     46|                }
  386|     46|                current_ptr = forwarding_ptr as *mut GcHeader<T>;
  387|       |            }
  388|       |
  389|       |            // Pointer compression optimization: if we followed forwarding pointers,
  390|       |            // update our internal pointer to point directly to the final target
  391|  1.14k|            if current_ptr != original_ptr {
  392|     10|                self.ptr.store(current_ptr, std::sync::atomic::Ordering::Release);
  393|  1.13k|            }
  394|       |
  395|       |            // Check if object is the free singleton (has been collected)
  396|  1.14k|            if current_ptr as *const GcHeader<()> == crate::types::FreeSingleton::instance() {
  397|      0|                return None;
  398|  1.14k|            }
  399|       |
  400|  1.14k|            Some(GcRef::new(&(*current_ptr).data))
  401|       |        }
  402|  1.14k|    }
  403|       |
  404|       |    /// Get a mutable reference to the object (if not being marked).
  405|  1.00k|    pub fn write(&self) -> Option<GcRefMut<'_, T>> {
  406|       |        unsafe {
  407|       |            // Follow forwarding pointers and implement pointer compression
  408|  1.00k|            let mut current_ptr = self.ptr.load(std::sync::atomic::Ordering::Acquire);
  409|  1.00k|            let original_ptr = current_ptr;
  410|       |            
  411|       |            loop {
  412|  1.00k|                let forwarding_ptr = (*current_ptr)
  413|  1.00k|                    .forwarding_ptr
  414|  1.00k|                    .load(std::sync::atomic::Ordering::Acquire);
  415|  1.00k|                if forwarding_ptr.is_null() {
  416|  1.00k|                    break;
  417|      0|                }
  418|      0|                current_ptr = forwarding_ptr as *mut GcHeader<T>;
  419|       |            }
  420|       |
  421|       |            // Pointer compression optimization: if we followed forwarding pointers,
  422|       |            // update our internal pointer to point directly to the final target
  423|  1.00k|            if current_ptr != original_ptr {
  424|      0|                self.ptr.store(current_ptr, std::sync::atomic::Ordering::Release);
  425|  1.00k|            }
  426|       |
  427|       |            // Check if object is the free singleton (has been collected)
  428|  1.00k|            if current_ptr as *const GcHeader<()> == crate::types::FreeSingleton::instance() {
  429|      0|                return None;
  430|  1.00k|            }
  431|       |
  432|       |            // Simple implementation - in a real GC we'd check marking phase
  433|  1.00k|            Some(GcRefMut::new(&mut (*current_ptr).data))
  434|       |        }
  435|  1.00k|    }
  436|       |}
  437|       |
  438|       |// Weak reference implementation
  439|       |impl<T: GcTrace + 'static> Weak<T> {
  440|       |    /// Creates a new weak reference to the target object using a custom allocator.
  441|       |    ///
  442|       |    /// Weak references do not prevent the target object from being garbage collected.
  443|       |    /// When the target is collected, the weak reference becomes invalid and
  444|       |    /// calls to `upgrade()` will return `None`.
  445|       |    ///
  446|       |    /// # Examples
  447|       |    ///
  448|       |    /// ```
  449|       |    /// use fugrip::{Weak, Gc};
  450|       |    /// use fugrip::CLASSIFIED_ALLOCATOR;
  451|       |    ///
  452|       |    /// let target = Gc::new(42i32);
  453|       |    /// let weak_ref = Weak::new(&target, &*CLASSIFIED_ALLOCATOR);
  454|       |    ///
  455|       |    /// // Weak reference can be upgraded to strong reference
  456|       |    /// let weak_guard = weak_ref.read().unwrap();
  457|       |    /// if let Some(strong_ref) = weak_guard.upgrade() {
  458|       |    ///     assert_eq!(*strong_ref.read().unwrap(), 42);
  459|       |    /// }
  460|       |    /// ```
  461|     15|    pub fn new<Alloc: AllocatorTrait>(target: &Gc<T>, allocator: &Alloc) -> Gc<Weak<T>> {
  462|     15|        let weak = Weak {
  463|     15|            target: AtomicPtr::new(target.as_ptr()),
  464|     15|            next_weak: AtomicPtr::new(std::ptr::null_mut()),
  465|     15|            prev_weak: AtomicPtr::new(std::ptr::null_mut()),
  466|     15|        };
  467|       |
  468|       |        // Add to target's weak list - create the weak reference first
  469|     15|        let weak_gc = allocator.allocate_classified(weak, ObjectClass::Weak);
  470|       |
  471|       |        // Now link it into the target's weak reference chain
  472|     15|        unsafe {
  473|     15|            Self::link_to_target_chain(target, &weak_gc);
  474|     15|        }
  475|       |
  476|     15|        weak_gc
  477|     15|    }
  478|       |
  479|       |    /// Sophisticated weak reference chain management
  480|       |    /// Links a weak reference into the target object's weak reference chain
  481|     15|    unsafe fn link_to_target_chain(target: &Gc<T>, weak_ref: &Gc<Weak<T>>)
  482|     15|    where
  483|     15|        T: GcTrace + 'static,
  484|       |    {
  485|     15|        let target_header = unsafe { &*target.as_ptr() };
  486|     15|        let weak_ptr = weak_ref.as_ptr() as *mut Weak<()>;
  487|       |
  488|       |        loop {
  489|       |            // Get current head of the weak reference chain
  490|     15|            let current_head = target_header.weak_ref_list.load(Ordering::Acquire);
  491|       |
  492|       |            // Set this weak reference as the new head
  493|     15|            let weak_data = unsafe { &(*weak_ref.as_ptr()).data };
  494|     15|            weak_data
  495|     15|                .next_weak
  496|     15|                .store(current_head as *mut Weak<T>, Ordering::Relaxed);
  497|     15|            weak_data
  498|     15|                .prev_weak
  499|     15|                .store(std::ptr::null_mut(), Ordering::Relaxed);
  500|       |
  501|       |            // If there was a previous head, update its prev pointer
  502|     15|            if !current_head.is_null() {
  503|      9|                let prev_head = unsafe { &*current_head };
  504|      9|                prev_head.prev_weak.store(weak_ptr, Ordering::Relaxed);
  505|      9|            }
                          ^6
  506|       |
  507|       |            // Atomically update the head pointer
  508|     15|            match target_header.weak_ref_list.compare_exchange_weak(
  509|     15|                current_head,
  510|     15|                weak_ptr,
  511|     15|                Ordering::Release,
  512|     15|                Ordering::Relaxed,
  513|     15|            ) {
  514|     15|                Ok(_) => break,     // Successfully linked
  515|      0|                Err(_) => continue, // Another thread updated the head, retry
  516|       |            }
  517|       |        }
  518|     15|    }
  519|       |
  520|       |    // invalidate_weak_chain moved to a separate impl block so it can be used
  521|       |    // without imposing `GcTrace` bounds on the type parameter.
  522|       |
  523|       |    /// Creates a new weak reference to the target object using the default allocator.
  524|       |    ///
  525|       |    /// This is a convenience method that uses the global `CLASSIFIED_ALLOCATOR`.
  526|       |    /// Equivalent to calling `Weak::new(target, &*CLASSIFIED_ALLOCATOR)`.
  527|       |    ///
  528|       |    /// # Examples
  529|       |    ///
  530|       |    /// Basic usage:
  531|       |    ///
  532|       |    /// ```
  533|       |    /// use fugrip::{Weak, Gc};
  534|       |    ///
  535|       |    /// let strong_ref = Gc::new("Hello".to_string());
  536|       |    /// let weak_ref = Weak::new_simple(&strong_ref);
  537|       |    ///
  538|       |    /// // Verify the weak reference works
  539|       |    /// let weak_guard = weak_ref.read().unwrap();
  540|       |    /// if let Some(upgraded) = weak_guard.upgrade() {
  541|       |    ///     assert_eq!(*upgraded.read().unwrap(), "Hello");
  542|       |    /// }
  543|       |    /// ```
  544|       |    ///
  545|       |    /// Multiple weak references to the same object:
  546|       |    ///
  547|       |    /// ```
  548|       |    /// use fugrip::{Weak, Gc};
  549|       |    ///
  550|       |    /// let original = Gc::new(vec![1, 2, 3, 4, 5]);
  551|       |    ///
  552|       |    /// let weak1 = Weak::new_simple(&original);
  553|       |    /// let weak2 = Weak::new_simple(&original);
  554|       |    ///
  555|       |    /// // Both weak references point to the same object
  556|       |    /// let guard1 = weak1.read().unwrap();
  557|       |    /// let guard2 = weak2.read().unwrap();
  558|       |    ///
  559|       |    /// if let (Some(ref1), Some(ref2)) = (guard1.upgrade(), guard2.upgrade()) {
  560|       |    ///     assert_eq!(ref1.as_ptr(), ref2.as_ptr());
  561|       |    ///     assert_eq!(*ref1.read().unwrap(), vec![1, 2, 3, 4, 5]);
  562|       |    /// }
  563|       |    /// ```
  564|       |    ///
  565|       |    /// Weak references surviving target collection:
  566|       |    ///
  567|       |    /// ```
  568|       |    /// use fugrip::{Weak, Gc};
  569|       |    ///
  570|       |    /// let weak_ref = {
  571|       |    ///     let target = Gc::new("temporary".to_string());
  572|       |    ///     Weak::new_simple(&target)
  573|       |    ///     // target goes out of scope, may be collected
  574|       |    /// };
  575|       |    ///
  576|       |    /// // Weak reference may now be invalid
  577|       |    /// let weak_guard = weak_ref.read().unwrap();
  578|       |    /// match weak_guard.upgrade() {
  579|       |    ///     Some(strong) => {
  580|       |    ///         println!("Target still alive: {}", *strong.read().unwrap());
  581|       |    ///     }
  582|       |    ///     None => {
  583|       |    ///         println!("Target was collected");
  584|       |    ///     }
  585|       |    /// }
  586|       |    /// ```
  587|     15|    pub fn new_simple(target: &Gc<T>) -> Gc<Weak<T>> {
  588|       |        use crate::memory::CLASSIFIED_ALLOCATOR;
  589|     15|        Self::new(target, &*CLASSIFIED_ALLOCATOR)
  590|     15|    }
  591|       |
  592|       |    /// Attempts to upgrade this weak reference to a strong reference.
  593|       |    ///
  594|       |    /// Returns `Some(Gc<T>)` if the target object is still alive and accessible,
  595|       |    /// or `None` if the target has been garbage collected or the weak reference
  596|       |    /// has been invalidated.
  597|       |    ///
  598|       |    /// # Examples
  599|       |    ///
  600|       |    /// Successful upgrade:
  601|       |    ///
  602|       |    /// ```
  603|       |    /// use fugrip::{Weak, Gc};
  604|       |    ///
  605|       |    /// let strong_ref = Gc::new(42i32);
  606|       |    /// let weak_ref = Weak::new_simple(&strong_ref);
  607|       |    ///
  608|       |    /// // While strong reference exists, upgrade should succeed
  609|       |    /// let weak_guard = weak_ref.read().unwrap();
  610|       |    /// if let Some(upgraded) = weak_guard.upgrade() {
  611|       |    ///     assert_eq!(*upgraded.read().unwrap(), 42);
  612|       |    /// }
  613|       |    /// ```
  614|       |    ///
  615|       |    /// Failed upgrade after collection:
  616|       |    ///
  617|       |    /// ```
  618|       |    /// use fugrip::{Weak, Gc};
  619|       |    ///
  620|       |    /// let weak_ref = {
  621|       |    ///     let temp = Gc::new("temporary".to_string());
  622|       |    ///     Weak::new_simple(&temp)
  623|       |    /// }; // temp goes out of scope
  624|       |    ///
  625|       |    /// // After potential garbage collection, upgrade may fail
  626|       |    /// let weak_guard = weak_ref.read().unwrap();
  627|       |    /// match weak_guard.upgrade() {
  628|       |    ///     Some(strong) => println!("Still alive: {}", *strong.read().unwrap()),
  629|       |    ///     None => println!("Object was collected"),
  630|       |    /// }
  631|       |    /// ```
  632|       |    ///
  633|       |    /// Using weak references to break cycles:
  634|       |    ///
  635|       |    /// ```
  636|       |    /// use fugrip::{Weak, Gc, GcTrace, SendPtr, GcHeader};
  637|       |    /// use std::cell::RefCell;
  638|       |    ///
  639|       |    /// struct Node {
  640|       |    ///     value: i32,
  641|       |    ///     parent: Option<Gc<Weak<Node>>>,
  642|       |    ///     children: Vec<Gc<Node>>,
  643|       |    /// }
  644|       |    ///
  645|       |    /// // Implement GcTrace for Node (unsafe trait requires `unsafe impl`)
  646|       |    /// unsafe impl GcTrace for Node {
  647|       |    ///     unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  648|       |    ///         if let Some(ref parent_weak) = self.parent {
  649|       |    ///             if let Some(parent_weak_ref) = parent_weak.read() {
  650|       |    ///                 unsafe { parent_weak_ref.trace(stack); }
  651|       |    ///             }
  652|       |    ///         }
  653|       |    ///         for child in &self.children {
  654|       |    ///             if let Some(child_ref) = child.read() {
  655|       |    ///                 unsafe { child_ref.trace(stack); }
  656|       |    ///             }
  657|       |    ///         }
  658|       |    ///     }
  659|       |    /// }
  660|       |    ///
  661|       |    /// let parent = Gc::new(Node {
  662|       |    ///     value: 1,
  663|       |    ///     parent: None,
  664|       |    ///     children: vec![],
  665|       |    /// });
  666|       |    ///
  667|       |    /// let parent_weak = Weak::new_simple(&parent);
  668|       |    /// let child = Gc::new(Node {
  669|       |    ///     value: 2,
  670|       |    ///     parent: Some(parent_weak),
  671|       |    ///     children: vec![],
  672|       |    /// });
  673|       |    ///
  674|       |    /// // Child can access parent through weak reference
  675|       |    /// let child_ref = child.read().unwrap();
  676|       |    /// if let Some(ref parent_weak_ref) = child_ref.parent {
  677|       |    ///     let weak_guard = parent_weak_ref.read().unwrap();
  678|       |    ///     if let Some(parent_ref) = weak_guard.upgrade() {
  679|       |    ///         assert_eq!(parent_ref.read().unwrap().value, 1);
  680|       |    ///     }
  681|       |    /// }
  682|       |    /// ```
  683|     13|    pub fn upgrade(&self) -> Option<Gc<T>> {
  684|     13|        let target_ptr = self.target.load(Ordering::Acquire);
  685|     13|        let free_singleton = FreeSingleton::instance();
  686|       |
  687|     13|        if target_ptr as *mut GcHeader<()> == free_singleton || target_ptr.is_null() {
  688|      0|            None
  689|       |        } else {
  690|     13|            Some(Gc {
  691|     13|                ptr: AtomicPtr::new(target_ptr),
  692|     13|                _phantom: PhantomData,
  693|     13|            })
  694|       |        }
  695|     13|    }
  696|       |}
  697|       |
  698|       |// Provide invalidate_weak_chain for Weak<()> without requiring GcTrace on T
  699|       |impl Weak<()> {
  700|       |    /// Invalidates all weak references in a chain (non-generic helper)
  701|       |    ///
  702|       |    /// # Safety
  703|       |    ///
  704|       |    /// The caller must ensure that:
  705|       |    /// - `head` points to a valid `Weak<()>` object or is null
  706|       |    /// - All weak references in the chain are valid and properly initialized
  707|       |    /// - The weak reference chain is not being concurrently modified by other threads
  708|       |    /// - No other code is accessing the weak references while this function runs
  709|       |    /// - The caller has exclusive access to modify the weak reference chain
  710|      0|    pub unsafe fn invalidate_weak_chain(head: *mut Weak<()>) {
  711|      0|        let mut current = head;
  712|       |
  713|      0|        while !current.is_null() {
  714|      0|            let weak_ref = unsafe { &*current };
  715|      0|            let next = weak_ref.next_weak.load(Ordering::Acquire);
  716|      0|
  717|      0|            // Clear the target pointer (weak reference becomes null)
  718|      0|            weak_ref
  719|      0|                .target
  720|      0|                .store(std::ptr::null_mut(), Ordering::Release);
  721|      0|
  722|      0|            // Clear chain pointers to avoid dangling references
  723|      0|            weak_ref
  724|      0|                .next_weak
  725|      0|                .store(std::ptr::null_mut(), Ordering::Relaxed);
  726|      0|            weak_ref
  727|      0|                .prev_weak
  728|      0|                .store(std::ptr::null_mut(), Ordering::Relaxed);
  729|      0|
  730|      0|            current = next;
  731|      0|        }
  732|      0|    }
  733|       |}
  734|       |
  735|       |// Census phase implementation
  736|       |impl CollectorState {
  737|      1|    pub fn census_phase(&self) {
  738|      1|        self.phase
  739|      1|            .store(CollectorPhase::Censusing as usize, Ordering::Release);
  740|       |
  741|       |        use crate::memory::CLASSIFIED_ALLOCATOR;
  742|       |        // ObjectClass is already imported at module level
  743|      1|        let worker_count = num_cpus::get();
  744|       |
  745|       |        // First, census all weak references themselves
  746|      1|        let weak_object_set = CLASSIFIED_ALLOCATOR.get_weak_object_set();
  747|       |
  748|       |        // Iterate over all weak references in parallel
  749|      3|        weak_object_set.iterate_parallel(worker_count, |weak_ptr| {
                      ^1              ^1               ^1
  750|       |            // Perform census operation on each weak reference
  751|       |            unsafe {
  752|      3|                let weak_header = &*(weak_ptr.as_ptr() as *mut GcHeader<Weak<()>>);
  753|      3|                let weak_data = &weak_header.data;
  754|      3|                let target = weak_data.target.load(Ordering::Acquire);
  755|       |
  756|      3|                if !target.is_null() && !(*target).mark_bit.load(Ordering::Acquire) {
  757|      3|                    // Target is not marked (dead), redirect weak reference to null
  758|      3|                    weak_data
  759|      3|                        .target
  760|      3|                        .store(std::ptr::null_mut(), Ordering::Release);
  761|      3|                }
                              ^0
  762|       |            }
  763|      3|        });
  764|       |
  765|       |        // Second, census objects with Census class (objects that may need weak ref census)
  766|      1|        let census_object_set = CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::Census);
  767|      1|        census_object_set.iterate_parallel(worker_count, |obj_ptr| {
                                                                                 ^0
  768|       |            // Inline the census logic to avoid borrowing issues
  769|       |            unsafe {
  770|      0|                let header = &*obj_ptr.as_ptr();
  771|       |
  772|       |                // If this object is not marked (dead), invalidate all weak references to it
  773|      0|                if !header.mark_bit.load(Ordering::Acquire) {
  774|      0|                    let weak_head = header.weak_ref_list.load(Ordering::Acquire);
  775|      0|                    if !weak_head.is_null() {
  776|      0|                        Weak::<()>::invalidate_weak_chain(weak_head);
  777|      0|                    }
  778|      0|                }
  779|       |            }
  780|      0|        });
  781|       |
  782|       |        // Third, census objects with CensusAndDestructor class
  783|      1|        let census_destructor_set =
  784|      1|            CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::CensusAndDestructor);
  785|      1|        census_destructor_set.iterate_parallel(worker_count, |obj_ptr| {
                                                                                     ^0
  786|       |            // Inline the census logic to avoid borrowing issues
  787|       |            unsafe {
  788|      0|                let header = &*obj_ptr.as_ptr();
  789|       |
  790|       |                // If this object is not marked (dead), invalidate all weak references to it
  791|      0|                if !header.mark_bit.load(Ordering::Acquire) {
  792|      0|                    let weak_head = header.weak_ref_list.load(Ordering::Acquire);
  793|      0|                    if !weak_head.is_null() {
  794|      0|                        Weak::<()>::invalidate_weak_chain(weak_head);
  795|      0|                    }
  796|      0|                }
  797|       |            }
  798|      0|        });
  799|      1|    }
  800|       |
  801|      0|    pub fn census_weak_reference(&self, weak_ptr: *mut GcHeader<()>) {
  802|       |        unsafe {
  803|      0|            let weak_header = &*(weak_ptr as *mut GcHeader<Weak<()>>);
  804|      0|            let weak_data = &weak_header.data;
  805|      0|            let target = weak_data.target.load(Ordering::Acquire);
  806|       |
  807|      0|            if !target.is_null() && !(*target).mark_bit.load(Ordering::Acquire) {
  808|      0|                // Target is not marked (dead), redirect weak reference to null
  809|      0|                weak_data
  810|      0|                    .target
  811|      0|                    .store(std::ptr::null_mut(), Ordering::Release);
  812|      0|            }
  813|       |        }
  814|      0|    }
  815|       |
  816|       |    /// Census objects that may have weak references pointing to them.
  817|       |    /// This method checks if the object itself is marked. If not,
  818|       |    /// it invalidates all weak references pointing to it.
  819|       |    ///
  820|       |    /// # Safety
  821|       |    ///
  822|       |    /// This function is unsafe because:
  823|       |    /// - The `obj_ptr` must point to a valid, initialized GcHeader
  824|       |    /// - The caller must ensure the object is not being accessed by other threads
  825|       |    /// - This should only be called during the census phase of garbage collection
  826|       |    /// - The weak reference chain must be valid if it exists
  827|      0|    pub unsafe fn census_object_with_weak_refs(&self, obj_ptr: *mut GcHeader<()>) {
  828|       |        unsafe {
  829|      0|            let header = &*obj_ptr;
  830|       |
  831|       |            // If this object is not marked (dead), invalidate all weak references to it
  832|      0|            if !header.mark_bit.load(Ordering::Acquire) {
  833|      0|                let weak_head = header.weak_ref_list.load(Ordering::Acquire);
  834|      0|                if !weak_head.is_null() {
  835|      0|                    Weak::<()>::invalidate_weak_chain(weak_head);
  836|      0|                }
  837|      0|            }
  838|       |        }
  839|      0|    }
  840|       |}
  841|       |
  842|       |pub struct SegmentBuffer {
  843|       |    pub current: *mut u8,
  844|       |    pub end: *mut u8,
  845|       |    pub segment_id: usize,
  846|       |}
  847|       |
  848|       |impl Default for SegmentBuffer {
  849|     26|    fn default() -> Self {
  850|     26|        SegmentBuffer {
  851|     26|            current: std::ptr::null_mut(),
  852|     26|            end: std::ptr::null_mut(),
  853|     26|            segment_id: 0,
  854|     26|        }
  855|     26|    }
  856|       |}
  857|       |
  858|       |// Object classification system for efficient parallel collection
  859|       |
  860|       |pub struct ClassifiedAllocator {
  861|       |    pub heaps: [SegmentedHeap; 6],   // One per ObjectClass
  862|       |    pub object_sets: [ObjectSet; 6], // For iteration during collection
  863|       |}
  864|       |
  865|       |impl ClassifiedAllocator {
  866|       |    /// Allocates a garbage-collected object with the specified classification.
  867|       |    ///
  868|       |    /// This method allocates memory for the object in the appropriate heap
  869|       |    /// based on its classification, enabling the garbage collector to handle
  870|       |    /// different object types efficiently.
  871|       |    ///
  872|       |    /// # Examples
  873|       |    ///
  874|       |    /// ```
  875|       |    /// use fugrip::{CLASSIFIED_ALLOCATOR, ObjectClass};
  876|       |    /// use fugrip::Gc;
  877|       |    ///
  878|       |    /// // Allocate a regular object
  879|       |    /// let regular = CLASSIFIED_ALLOCATOR.allocate_classified(
  880|       |    ///     42i32,
  881|       |    ///     ObjectClass::Default
  882|       |    /// );
  883|       |    /// assert_eq!(*regular.read().unwrap(), 42);
  884|       |    ///
  885|       |    /// // Allocate an object that needs finalization
  886|       |    /// let finalizable = CLASSIFIED_ALLOCATOR.allocate_classified(
  887|       |    ///     "cleanup me".to_string(),
  888|       |    ///     ObjectClass::Finalizer
  889|       |    /// );
  890|       |    /// assert_eq!(*finalizable.read().unwrap(), "cleanup me");
  891|       |    /// ```
  892|     16|    pub fn allocate_classified<T: GcTrace + 'static>(&self, value: T, class: ObjectClass) -> Gc<T> {
  893|     16|        let heap_index = class as usize;
  894|     16|        let gc_ptr = self.heaps[heap_index].allocate(value);
  895|       |
  896|       |        // Register in appropriate object set for collection
  897|     16|        unsafe { self.object_sets[heap_index].register(gc_ptr.as_ptr() as *mut GcHeader<()>) };
  898|       |
  899|     16|        gc_ptr
  900|     16|    }
  901|       |}
  902|       |
  903|       |// Object sets for efficient iteration during collection phases
  904|       |pub struct ObjectSet {
  905|       |    pub objects: parking_lot::RwLock<Vec<SendPtr<GcHeader<()>>>>,
  906|       |    pub iteration_state: parking_lot::Mutex<IterationState>,
  907|       |}
  908|       |
  909|       |pub struct IterationState {
  910|       |    _current_index: usize,
  911|       |    _total_size: usize,
  912|       |    _is_iterating: bool,
  913|       |}
  914|       |
  915|       |impl Default for ObjectSet {
  916|      0|    fn default() -> Self {
  917|      0|        Self::new()
  918|      0|    }
  919|       |}
  920|       |
  921|       |impl ObjectSet {
  922|     24|    pub fn new() -> Self {
  923|     24|        ObjectSet {
  924|     24|            objects: parking_lot::RwLock::new(Vec::new()),
  925|     24|            iteration_state: parking_lot::Mutex::new(IterationState {
  926|     24|                _current_index: 0,
  927|     24|                _total_size: 0,
  928|     24|                _is_iterating: false,
  929|     24|            }),
  930|     24|        }
  931|     24|    }
  932|       |
  933|       |    /// # Safety
  934|       |    /// The caller must ensure `ptr` is a valid pointer to an initialized
  935|       |    /// `GcHeader` that will remain valid for the lifetime of registration.
  936|     16|    pub unsafe fn register(&self, ptr: *mut GcHeader<()>) {
  937|     16|        self.objects.write().push(unsafe { SendPtr::new(ptr) });
  938|     16|    }
  939|       |
  940|       |    /// Iterate over all objects in parallel, applying the given function to each.
  941|       |    ///
  942|       |    /// This method splits the object set across multiple worker threads for
  943|       |    /// efficient parallel processing during garbage collection phases.
  944|       |    ///
  945|       |    /// # Parameters
  946|       |    ///
  947|       |    /// * `worker_count` - Number of worker threads to use
  948|       |    /// * `func` - Function to apply to each object
  949|       |    ///
  950|       |    /// # Examples
  951|       |    ///
  952|       |    /// ```
  953|       |    /// use fugrip::{ObjectSet, CLASSIFIED_ALLOCATOR, ObjectClass};
  954|       |    /// use fugrip::Gc;
  955|       |    /// use std::sync::atomic::{AtomicUsize, Ordering};
  956|       |    /// use std::sync::Arc;
  957|       |    ///
  958|       |    /// // Create some objects
  959|       |    /// for i in 0..10 {
  960|       |    ///     CLASSIFIED_ALLOCATOR.allocate_classified(i, ObjectClass::Default);
  961|       |    /// }
  962|       |    ///
  963|       |    /// // Get the object set and count processed objects
  964|       |    /// let object_set = CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::Default);
  965|       |    /// let counter = Arc::new(AtomicUsize::new(0));
  966|       |    /// let counter_clone = counter.clone();
  967|       |    ///
  968|       |    /// // Process objects in parallel
  969|       |    /// object_set.iterate_parallel(2, move |_obj_ptr| {
  970|       |    ///     counter_clone.fetch_add(1, Ordering::Relaxed);
  971|       |    /// });
  972|       |    ///
  973|       |    /// assert!(counter.load(Ordering::Relaxed) >= 10);
  974|       |    /// ```
  975|      4|    pub fn iterate_parallel<F>(&self, worker_count: usize, func: F)
  976|      4|    where
  977|      4|        F: Fn(SendPtr<GcHeader<()>>) + Send + Sync + 'static,
  978|       |    {
  979|      4|        let objects = self.objects.read();
  980|      4|        let total_objects = objects.len();
  981|       |
  982|      4|        if total_objects == 0 {
  983|      2|            return;
  984|      2|        }
  985|       |
  986|      2|        let chunk_size = total_objects.div_ceil(worker_count);
  987|      2|        let func = std::sync::Arc::new(func);
  988|      2|        let mut handles = Vec::new();
  989|       |
  990|      6|        for worker_id in 0..worker_count {
                                          ^2
  991|      6|            let start = worker_id * chunk_size;
  992|      6|            let end = std::cmp::min(start + chunk_size, total_objects);
  993|       |
  994|      6|            if start >= total_objects {
  995|      2|                break;
  996|      4|            }
  997|       |
  998|      4|            let worker_objects: Vec<SendPtr<GcHeader<()>>> = objects[start..end].to_vec();
  999|      4|            let worker_func = func.clone();
 1000|       |
 1001|      4|            let handle = std::thread::spawn(move || {
 1002|      8|                for obj_ptr in worker_objects {
                                  ^4
 1003|      4|                    worker_func(obj_ptr);
 1004|      4|                }
 1005|      4|            });
 1006|       |
 1007|      4|            handles.push(handle);
 1008|       |        }
 1009|       |
 1010|       |        // Wait for all workers to complete
 1011|      6|        for handle in handles {
                          ^4
 1012|      4|            handle.join().expect("Worker thread panicked");
 1013|      4|        }
 1014|      4|    }
 1015|       |
 1016|       |    /// Returns the number of objects currently in this set.
 1017|       |    ///
 1018|       |    /// This method provides a snapshot count of registered objects,
 1019|       |    /// useful for monitoring and debugging garbage collection behavior.
 1020|       |    ///
 1021|       |    /// # Examples
 1022|       |    ///
 1023|       |    /// ```
 1024|       |    /// use fugrip::{CLASSIFIED_ALLOCATOR, ObjectClass};
 1025|       |    ///
 1026|       |    /// // Initially empty
 1027|       |    /// let object_set = CLASSIFIED_ALLOCATOR.get_object_set(ObjectClass::Default);
 1028|       |    /// let initial_count = object_set.get_object_count();
 1029|       |    ///
 1030|       |    /// // Allocate some objects
 1031|       |    /// CLASSIFIED_ALLOCATOR.allocate_classified(1i32, ObjectClass::Default);
 1032|       |    /// CLASSIFIED_ALLOCATOR.allocate_classified(2i32, ObjectClass::Default);
 1033|       |    ///
 1034|       |    /// // Count should increase
 1035|       |    /// let new_count = object_set.get_object_count();
 1036|       |    /// assert!(new_count >= initial_count + 2);
 1037|       |    /// ```
 1038|      2|    pub fn get_object_count(&self) -> usize {
 1039|      2|        self.objects.read().len()
 1040|      2|    }
 1041|       |}
 1042|       |
 1043|       |impl Default for ClassifiedAllocator {
 1044|      0|    fn default() -> Self {
 1045|      0|        Self::new()
 1046|      0|    }
 1047|       |}
 1048|       |
 1049|       |impl ClassifiedAllocator {
 1050|      4|    pub fn new() -> Self {
 1051|      4|        ClassifiedAllocator {
 1052|      4|            heaps: [
 1053|      4|                SegmentedHeap::new(), // Default
 1054|      4|                SegmentedHeap::new(), // Destructor
 1055|      4|                SegmentedHeap::new(), // Census
 1056|      4|                SegmentedHeap::new(), // CensusAndDestructor
 1057|      4|                SegmentedHeap::new(), // Finalizer
 1058|      4|                SegmentedHeap::new(), // Weak
 1059|      4|            ],
 1060|      4|            object_sets: [
 1061|      4|                ObjectSet::new(),
 1062|      4|                ObjectSet::new(),
 1063|      4|                ObjectSet::new(),
 1064|      4|                ObjectSet::new(),
 1065|      4|                ObjectSet::new(),
 1066|      4|                ObjectSet::new(),
 1067|      4|            ],
 1068|      4|        }
 1069|      4|    }
 1070|       |
 1071|       |    /// Get the object set for a specific object class
 1072|      3|    pub fn get_object_set(&self, class: ObjectClass) -> &ObjectSet {
 1073|      3|        &self.object_sets[class as usize]
 1074|      3|    }
 1075|       |
 1076|       |    /// Get the weak reference object set specifically for census operations
 1077|      2|    pub fn get_weak_object_set(&self) -> &ObjectSet {
 1078|      2|        &self.object_sets[ObjectClass::Weak as usize]
 1079|      2|    }
 1080|       |}
 1081|       |
 1082|       |// Implement AllocatorTrait for ClassifiedAllocator
 1083|       |impl crate::interfaces::AllocatorTrait for ClassifiedAllocator {
 1084|     15|    fn allocate_classified<T: GcTrace + 'static>(&self, value: T, class: ObjectClass) -> Gc<T> {
 1085|       |        // Call the inherent method using UFCS to avoid recursion
 1086|     15|        ClassifiedAllocator::allocate_classified(self, value, class)
 1087|     15|    }
 1088|       |
 1089|      0|    fn bytes_allocated(&self) -> usize {
 1090|       |        // Approximate by summing segment sizes
 1091|      0|        let segments = self.heaps[0].segments.lock().unwrap();
 1092|      0|        segments
 1093|      0|            .iter()
 1094|      0|            .map(|s| {
 1095|      0|                let start = s.memory.as_ptr() as usize;
 1096|      0|                let end = s.end_ptr.load(std::sync::atomic::Ordering::Relaxed) as usize;
 1097|      0|                end - start
 1098|      0|            })
 1099|      0|            .sum()
 1100|      0|    }
 1101|       |
 1102|      0|    fn object_count(&self) -> usize {
 1103|      0|        self.object_sets
 1104|      0|            .iter()
 1105|      0|            .map(|set| set.get_object_count())
 1106|      0|            .sum()
 1107|      0|    }
 1108|       |}
 1109|       |
 1110|       |// Global classified allocator instance
 1111|       |pub static CLASSIFIED_ALLOCATOR: once_cell::sync::Lazy<ClassifiedAllocator> =
 1112|       |    once_cell::sync::Lazy::new(ClassifiedAllocator::new);
 1113|       |
 1114|       |// Root set management
 1115|       |
 1116|       |pub static ROOTS: once_cell::sync::Lazy<Mutex<Vec<SendPtr<GcHeader<()>>>>> =
 1117|      0|    once_cell::sync::Lazy::new(|| Mutex::new(Vec::new()));
 1118|       |
 1119|       |/// Registers a garbage-collected object as a global root.
 1120|       |///
 1121|       |/// Root objects are never collected by the garbage collector, ensuring they
 1122|       |/// remain accessible throughout the program's lifetime. This is useful for
 1123|       |/// global state and long-lived data structures.
 1124|       |///
 1125|       |/// # Examples
 1126|       |///
 1127|       |/// ```
 1128|       |/// use fugrip::{Gc, register_root};
 1129|       |///
 1130|       |/// // Create a global configuration object
 1131|       |/// let config = Gc::new("app_config".to_string());
 1132|       |///
 1133|       |/// // Register it as a root so it's never collected
 1134|       |/// register_root(&config);
 1135|       |///
 1136|       |/// // The config will remain accessible even after many GC cycles
 1137|       |/// assert_eq!(*config.read().unwrap(), "app_config");
 1138|       |/// ```
 1139|      0|pub fn register_root<T>(gc: &Gc<T>)
 1140|      0|where
 1141|      0|    T: GcTrace + 'static,
 1142|       |{
 1143|      0|    ROOTS
 1144|      0|        .lock()
 1145|      0|        .unwrap()
 1146|      0|        .push(unsafe { SendPtr::new(gc.as_ptr() as *mut GcHeader<()>) });
 1147|      0|}
 1148|       |
 1149|       |/// Stack scanning - delegates to the sophisticated collector implementation
 1150|      0|pub fn scan_stacks(_mark_stack: &mut [SendPtr<GcHeader<()>>]) {
 1151|       |    // Use the sophisticated stack scanning from CollectorState
 1152|       |    // This includes conservative scanning of thread stacks, data segments, and BSS
 1153|      0|    COLLECTOR.mark_global_roots();
 1154|       |
 1155|       |    // The collector's mark_global_roots() method populates its internal global_mark_stack
 1156|       |    // We could copy those roots to our mark_stack if needed, but typically the collector
 1157|       |    // manages its own marking process through the global_mark_stack
 1158|      0|}

/home/martin/git/fugrip/src/traits.rs:
    1|       |// From gc_ref_wrappers.rs: trait impls
    2|       |use crate::types::{Gc, Weak, WeakRef};
    3|       |// ...existing code...
    4|       |
    5|       |// ...existing code...
    6|       |use crate::types::{GcHeader, SendPtr};
    7|       |
    8|       |/// Trait for objects that can be traced during garbage collection.
    9|       |///
   10|       |/// Types implementing this trait can be stored in the garbage-collected heap.
   11|       |/// The `trace` method is called during the marking phase to identify all
   12|       |/// reachable objects in the object graph.
   13|       |///
   14|       |/// # Safety
   15|       |///
   16|       |/// This method is `unsafe` because it deals with raw pointers and is called
   17|       |/// during garbage collection when normal borrowing rules may not apply.
   18|       |///
   19|       |/// # Examples
   20|       |///
   21|       |/// Implementing `GcTrace` for a custom type:
   22|       |///
   23|       |/// ```
   24|       |/// use fugrip::{GcTrace, Gc, SendPtr, GcHeader};
   25|       |///
   26|       |/// struct Node {
   27|       |///     value: i32,
   28|       |///     next: Option<Gc<Node>>,
   29|       |/// }
   30|       |///
   31|       |/// # // We can't actually implement GcTrace for Node in a doctest because it would
   32|       |/// # // violate the orphan rule, but this shows the pattern
   33|       |/// # trait MockGcTrace {
   34|       |/// #     unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>);
   35|       |/// # }
   36|       |/// # impl MockGcTrace for Node {
   37|       |/// #     unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
   38|       |/// #         // In a real implementation, this would trace the next pointer
   39|       |/// #         // if let Some(ref next) = self.next {
   40|       |/// #         //     next.trace(stack);
   41|       |/// #         // }
   42|       |/// #     }
   43|       |/// # }
   44|       |///
   45|       |/// // The implementation would look like this:
   46|       |/// // unsafe impl GcTrace for Node {
   47|       |/// //     unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
   48|       |/// //         if let Some(ref next) = self.next {
   49|       |/// //             next.trace(stack);
   50|       |/// //         }
   51|       |/// //     }
   52|       |/// // }
   53|       |/// ```
   54|       |pub unsafe trait GcTrace {
   55|       |    /// Traces all garbage-collected references within this object.
   56|       |    ///
   57|       |    /// This method should call `trace` on all `Gc<T>` fields within the object,
   58|       |    /// allowing the garbage collector to discover all reachable objects.
   59|       |    ///
   60|       |    /// # Safety
   61|       |    ///
   62|       |    /// This method is called during garbage collection and must not:
   63|       |    /// - Allocate new objects
   64|       |    /// - Access objects that might be collected
   65|       |    /// - Cause data races with concurrent collectors
   66|       |    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>);
   67|       |}
   68|       |
   69|       |// Provide GcTrace implementations for common primitive and standard types
   70|       |unsafe impl GcTrace for i32 {
   71|     12|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   72|       |}
   73|       |
   74|       |unsafe impl GcTrace for usize {
   75|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   76|       |}
   77|       |
   78|       |unsafe impl GcTrace for String {
   79|      1|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   80|       |}
   81|       |
   82|       |unsafe impl GcTrace for () {
   83|      1|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   84|       |}
   85|       |
   86|       |unsafe impl<T> GcTrace for crate::types::Weak<T> {
   87|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {
   88|       |        // Weak references don't trace their targets during marking
   89|      0|    }
   90|       |}
   91|       |
   92|       |// Additional primitive type implementations
   93|       |unsafe impl GcTrace for u8 {
   94|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   95|       |}
   96|       |
   97|       |unsafe impl GcTrace for i8 {
   98|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
   99|       |}
  100|       |
  101|       |unsafe impl GcTrace for u16 {
  102|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  103|       |}
  104|       |
  105|       |unsafe impl GcTrace for i16 {
  106|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  107|       |}
  108|       |
  109|       |unsafe impl GcTrace for u32 {
  110|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  111|       |}
  112|       |
  113|       |unsafe impl GcTrace for i64 {
  114|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  115|       |}
  116|       |
  117|       |unsafe impl GcTrace for u64 {
  118|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  119|       |}
  120|       |
  121|       |unsafe impl GcTrace for isize {
  122|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  123|       |}
  124|       |
  125|       |unsafe impl GcTrace for f32 {
  126|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  127|       |}
  128|       |
  129|       |unsafe impl GcTrace for f64 {
  130|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  131|       |}
  132|       |
  133|       |unsafe impl GcTrace for bool {
  134|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  135|       |}
  136|       |
  137|       |unsafe impl GcTrace for char {
  138|      0|    unsafe fn trace(&self, _stack: &mut Vec<SendPtr<GcHeader<()>>>) {}
  139|       |}
  140|       |
  141|       |// Container implementations
  142|       |unsafe impl<T: GcTrace> GcTrace for Vec<T> {
  143|      1|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  144|      3|        for item in self.iter() {
                                  ^1   ^1
  145|      3|            unsafe { item.trace(stack); }
  146|       |        }
  147|      1|    }
  148|       |}
  149|       |
  150|       |unsafe impl<T: GcTrace> GcTrace for Option<T> {
  151|      2|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  152|      2|        if let Some(item) = self {
                                  ^1
  153|      1|            unsafe { item.trace(stack); }
  154|      1|        }
  155|      2|    }
  156|       |}
  157|       |
  158|       |unsafe impl<T: GcTrace> GcTrace for Box<T> {
  159|      1|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  160|      1|        unsafe { (**self).trace(stack); }
  161|      1|    }
  162|       |}
  163|       |
  164|       |// Tuple implementations
  165|       |unsafe impl<T: GcTrace> GcTrace for (T,) {
  166|      1|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  167|      1|        unsafe { self.0.trace(stack); }
  168|      1|    }
  169|       |}
  170|       |
  171|       |unsafe impl<T: GcTrace, U: GcTrace> GcTrace for (T, U) {
  172|      1|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  173|      1|        unsafe { 
  174|      1|            self.0.trace(stack);
  175|      1|            self.1.trace(stack);
  176|      1|        }
  177|      1|    }
  178|       |}
  179|       |
  180|       |unsafe impl<T: GcTrace, U: GcTrace, V: GcTrace> GcTrace for (T, U, V) {
  181|      1|    unsafe fn trace(&self, stack: &mut Vec<SendPtr<GcHeader<()>>>) {
  182|      1|        unsafe {
  183|      1|            self.0.trace(stack);
  184|      1|            self.1.trace(stack);
  185|      1|            self.2.trace(stack);
  186|      1|        }
  187|      1|    }
  188|       |}
  189|       |
  190|       |/// Trait for objects that can be finalized before collection.
  191|       |///
  192|       |/// Objects implementing this trait have their `finalize` method called
  193|       |/// when they are about to be garbage collected, allowing for cleanup
  194|       |/// operations like closing files or network connections.
  195|       |///
  196|       |/// # Examples
  197|       |///
  198|       |/// ```
  199|       |/// use fugrip::traits::GcFinalize;
  200|       |/// use std::sync::Arc;
  201|       |/// use std::sync::atomic::{AtomicBool, Ordering};
  202|       |///
  203|       |/// struct FileResource {
  204|       |///     name: String,
  205|       |///     closed: Arc<AtomicBool>,
  206|       |/// }
  207|       |///
  208|       |/// impl GcFinalize for FileResource {
  209|       |///     fn finalize(&mut self) {
  210|       |///         // Clean up the resource
  211|       |///         println!("Closing file: {}", self.name);
  212|       |///         self.closed.store(true, Ordering::SeqCst);
  213|       |///     }
  214|       |/// }
  215|       |///
  216|       |/// let closed_flag = Arc::new(AtomicBool::new(false));
  217|       |/// let mut resource = FileResource {
  218|       |///     name: "test.txt".to_string(),
  219|       |///     closed: closed_flag.clone(),
  220|       |/// };
  221|       |///
  222|       |/// // This would normally be called automatically by the GC
  223|       |/// resource.finalize();
  224|       |/// assert!(closed_flag.load(Ordering::SeqCst));
  225|       |/// ```
  226|       |pub trait GcFinalize {
  227|       |    /// Called when the object is about to be garbage collected.
  228|       |    ///
  229|       |    /// This method should perform any necessary cleanup operations.
  230|       |    /// It should not panic or perform operations that could fail.
  231|       |    fn finalize(&mut self);
  232|       |}
  233|       |
  234|       |/// Trait for objects that can be dropped with custom cleanup logic.
  235|       |///
  236|       |/// This trait provides a safe alternative to implementing `Drop` for
  237|       |/// garbage-collected objects, as the standard `Drop` trait may not
  238|       |/// be called at predictable times due to the garbage collector.
  239|       |pub trait GcDrop {
  240|       |    /// Called when the object is being dropped by the garbage collector.
  241|       |    ///
  242|       |    /// This method should perform any necessary cleanup operations
  243|       |    /// that don't require the object to remain alive.
  244|       |    fn gc_drop(&mut self);
  245|       |}
  246|       |
  247|       |/// Trait for objects that need special handling during census phase.
  248|       |///
  249|       |/// Objects implementing this trait are processed during the census phase
  250|       |/// of garbage collection to handle weak reference invalidation and
  251|       |/// similar operations that require coordination with the collector.
  252|       |///
  253|       |/// # Examples
  254|       |///
  255|       |/// ```
  256|       |/// use fugrip::traits::GcCensus;
  257|       |/// use std::sync::atomic::{AtomicUsize, Ordering};
  258|       |///
  259|       |/// struct CacheManager {
  260|       |///     cache_size: AtomicUsize,
  261|       |///     max_size: usize,
  262|       |/// }
  263|       |///
  264|       |/// impl GcCensus for CacheManager {
  265|       |///     fn census(&self) -> bool {
  266|       |///         let current_size = self.cache_size.load(Ordering::Acquire);
  267|       |///         if current_size > self.max_size {
  268|       |///             // Indicate that cache needs cleanup
  269|       |///             println!("Cache size {} exceeds max {}, needs cleanup", current_size, self.max_size);
  270|       |///             true
  271|       |///         } else {
  272|       |///             false
  273|       |///         }
  274|       |///     }
  275|       |/// }
  276|       |///
  277|       |/// let manager = CacheManager {
  278|       |///     cache_size: AtomicUsize::new(1000),
  279|       |///     max_size: 500,
  280|       |/// };
  281|       |///
  282|       |/// // This would be called during GC census phase
  283|       |/// let needs_cleanup = manager.census();
  284|       |/// assert!(needs_cleanup); // Cache exceeds max size
  285|       |/// ```
  286|       |pub trait GcCensus {
  287|       |    /// Called during the census phase of garbage collection.
  288|       |    ///
  289|       |    /// This method can be used to update weak references, clean up
  290|       |    /// expired caches, or perform other census-related operations.
  291|       |    fn census(&self) -> bool;
  292|       |}
  293|       |
  294|       |/// Trait for objects that can be revived during finalization.
  295|       |///
  296|       |/// Objects implementing this trait may be "revived" (marked as reachable again)
  297|       |/// during the revival phase if their finalizer makes them reachable again.
  298|       |pub trait GcRevive {
  299|       |    /// Called during the revival phase to determine if this object should be revived.
  300|       |    ///
  301|       |    /// Returns `true` if the object should be kept alive for another collection cycle.
  302|       |    fn should_revive(&self) -> bool;
  303|       |}
  304|       |
  305|       |/// Trait for custom marking strategies during garbage collection.
  306|       |///
  307|       |/// Advanced users can implement this trait to provide custom marking
  308|       |/// logic for complex data structures or optimization purposes.
  309|       |pub trait GcMark {
  310|       |    /// Custom marking logic for this object.
  311|       |    ///
  312|       |    /// # Safety
  313|       |    ///
  314|       |    /// This method must correctly mark all reachable objects and not
  315|       |    /// cause data races with concurrent markers.
  316|       |    unsafe fn mark(&self, marker: &dyn GcMarker);
  317|       |}
  318|       |
  319|       |/// Trait for objects that provide custom marking functionality.
  320|       |///
  321|       |/// This trait is implemented by the garbage collector's marking subsystem
  322|       |/// and provided to objects that implement `GcMark`.
  323|       |pub trait GcMarker {
  324|       |    /// Mark an object as reachable.
  325|       |    ///
  326|       |    /// # Safety
  327|       |    ///
  328|       |    /// The pointer must point to a valid garbage-collected object.
  329|       |    unsafe fn mark_object(&self, ptr: *mut GcHeader<()>);
  330|       |
  331|       |    /// Check if an object is already marked.
  332|       |    ///
  333|       |    /// # Safety
  334|       |    ///
  335|       |    /// The pointer must point to a valid garbage-collected object.
  336|       |    unsafe fn is_marked(&self, ptr: *mut GcHeader<()>) -> bool;
  337|       |}
  338|       |
  339|       |/// Trait for objects that can provide heap statistics.
  340|       |///
  341|       |/// This trait allows objects to report their memory usage and
  342|       |/// contribute to overall heap statistics.
  343|       |pub trait GcStats {
  344|       |    /// Returns the size of this object in bytes.
  345|       |    fn size_bytes(&self) -> usize;
  346|       |
  347|       |    /// Returns the number of child objects referenced by this object.
  348|       |    fn child_count(&self) -> usize;
  349|       |
  350|       |    /// Returns additional statistics specific to this object type.
  351|      0|    fn custom_stats(&self) -> std::collections::HashMap<String, u64> {
  352|      0|        std::collections::HashMap::new()
  353|      0|    }
  354|       |}
  355|       |
  356|       |/// Trait for objects that can be visited during heap traversal.
  357|       |///
  358|       |/// This trait is used for debugging, profiling, and analysis tools
  359|       |/// that need to traverse the entire heap structure.
  360|       |pub trait GcVisitable {
  361|       |    /// Accept a visitor for this object.
  362|       |    fn accept_visitor<V: GcVisitor>(&self, visitor: &mut V);
  363|       |}
  364|       |
  365|       |/// Trait for visitors that can process objects during heap traversal.
  366|       |///
  367|       |/// This trait is used in conjunction with `GcVisitable` to implement
  368|       |/// the visitor pattern for heap analysis.
  369|       |pub trait GcVisitor {
  370|       |    /// Visit an object during heap traversal.
  371|       |    fn visit_object(&mut self, ptr: *const GcHeader<()>, size: usize);
  372|       |
  373|       |    /// Visit a reference between objects.
  374|       |    fn visit_reference(&mut self, from: *const GcHeader<()>, to: *const GcHeader<()>);
  375|       |}
  376|       |
  377|       |/// Trait for objects that can be serialized from the garbage-collected heap.
  378|       |///
  379|       |/// This trait provides a safe way to serialize objects that may contain
  380|       |/// circular references or other complex structures.
  381|       |pub trait GcSerialize {
  382|       |    /// Serialize this object to the given writer.
  383|       |    ///
  384|       |    /// The serializer should handle circular references appropriately.
  385|       |    fn serialize<W: std::io::Write>(
  386|       |        &self,
  387|       |        writer: &mut W,
  388|       |        context: &mut dyn GcSerializeContext,
  389|       |    ) -> std::io::Result<()>;
  390|       |}
  391|       |
  392|       |/// Context for serialization operations that handles circular references.
  393|       |pub trait GcSerializeContext {
  394|       |    /// Check if an object has already been serialized.
  395|       |    fn is_visited(&self, ptr: *const GcHeader<()>) -> bool;
  396|       |
  397|       |    /// Mark an object as visited during serialization.
  398|       |    fn mark_visited(&mut self, ptr: *const GcHeader<()>);
  399|       |
  400|       |    /// Get a unique identifier for an object.
  401|       |    fn get_object_id(&self, ptr: *const GcHeader<()>) -> u64;
  402|       |}
  403|       |
  404|       |/// Trait for objects that can be deserialized into the garbage-collected heap.
  405|       |pub trait GcDeserialize: Sized {
  406|       |    /// Deserialize an object from the given reader.
  407|       |    fn deserialize<R: std::io::Read>(
  408|       |        reader: &mut R,
  409|       |        context: &mut dyn GcDeserializeContext,
  410|       |    ) -> std::io::Result<Self>;
  411|       |}
  412|       |
  413|       |/// Context for deserialization operations.
  414|       |pub trait GcDeserializeContext {
  415|       |    /// Register a deserialized object with a given ID.
  416|       |    fn register_object(&mut self, id: u64, ptr: *mut GcHeader<()>);
  417|       |
  418|       |    /// Get a previously deserialized object by ID.
  419|       |    fn get_object(&self, id: u64) -> Option<*mut GcHeader<()>>;
  420|       |}

/home/martin/git/fugrip/src/types.rs:
    1|       |use crate::Finalizable;
    2|       |// From gc_ref_wrappers.rs: types only
    3|       |
    4|       |/// Guard types for read/write access control
    5|       |pub struct ReadGuard;
    6|       |pub struct WriteGuard;
    7|       |
    8|       |/// A finalizable wrapper used for tests and examples.
    9|       |pub struct FinalizableObject<T: Finalizable> {
   10|       |    pub data: T,
   11|       |    pub finalize_state: std::sync::atomic::AtomicU8,
   12|       |}
   13|       |use std::marker::PhantomData;
   14|       |use std::sync::atomic::{AtomicBool, AtomicPtr};
   15|       |use thiserror::Error;
   16|       |
   17|       |#[derive(Error, Debug)]
   18|       |pub enum GcError {
   19|       |    #[error("Access to freed object")]
   20|       |    AccessToFreedObject,
   21|       |    #[error("Allocation failed")]
   22|       |    AllocationFailed,
   23|       |    #[error("Collection in progress")]
   24|       |    CollectionInProgress,
   25|       |}
   26|       |
   27|       |pub type GcResult<T> = Result<T, GcError>;
   28|       |
   29|       |#[derive(Debug, Clone, Copy, PartialEq, Eq)]
   30|       |pub enum ObjectType {
   31|       |    Regular,
   32|       |    Weak,
   33|       |    WeakMap,
   34|       |    ExactPtrTable,
   35|       |    Finalizable,
   36|       |}
   37|       |
   38|       |/// Classification system for garbage-collected objects.
   39|       |#[derive(Debug, Clone, Copy, PartialEq, Eq)]
   40|       |pub enum ObjectClass {
   41|       |    Default,
   42|       |    Destructor,
   43|       |    Census,
   44|       |    CensusAndDestructor,
   45|       |    Finalizer,
   46|       |    Weak,
   47|       |}
   48|       |
   49|       |#[derive(Debug, Clone, Copy, PartialEq)]
   50|       |pub enum CollectorPhase {
   51|       |    Waiting,
   52|       |    Marking,
   53|       |    Censusing,
   54|       |    Reviving,
   55|       |    Remarking,
   56|       |    Recensusing,
   57|       |    Sweeping,
   58|       |}
   59|       |
   60|       |/// Thread-safe pointer wrapper for garbage collection.
   61|       |///
   62|       |/// `SendPtr<T>` is a wrapper around a raw pointer that implements `Send` and `Sync`,
   63|       |/// allowing it to be safely passed between threads. This is used internally by the
   64|       |/// garbage collector to manage object references across thread boundaries.
   65|       |///
   66|       |/// # Examples
   67|       |/// Thread-safe pointer wrapper for garbage collection.
   68|       |///
   69|       |/// `SendPtr<T>` is a small wrapper around a raw pointer that is `Send`/`Sync`.
   70|       |/// It is primarily used internally but can be constructed in unsafe contexts.
   71|       |///
   72|       |/// # Examples
   73|       |///
   74|       |/// ```
   75|       |/// use fugrip::SendPtr;
   76|       |///
   77|       |/// let x = Box::into_raw(Box::new(10i32));
   78|       |/// // Safety: `x` is a valid pointer here and we immediately convert it back.
   79|       |/// let sp = unsafe { SendPtr::new(x) };
   80|       |/// assert_eq!(unsafe { *sp.as_ptr() }, 10);
   81|       |/// // Clean up to avoid leak in doctest
   82|       |/// unsafe { Box::from_raw(x); }
   83|       |/// ```
   84|       |#[derive(Debug)]
   85|       |pub struct SendPtr<T> {
   86|       |    ptr: *mut T,
   87|       |}
   88|       |
   89|       |unsafe impl<T> Send for SendPtr<T> {}
   90|       |unsafe impl<T> Sync for SendPtr<T> {}
   91|       |
   92|       |impl<T> SendPtr<T> {
   93|       |    /// Creates a new SendPtr from a raw pointer.
   94|       |    ///
   95|       |    /// # Safety
   96|       |    ///
   97|       |    /// The caller must ensure that:
   98|       |    /// - `ptr` points to a valid object of type `T`
   99|       |    /// - The object will remain valid for the lifetime of this `SendPtr`
  100|       |    /// - The pointer is safe to send across thread boundaries
  101|    228|    pub unsafe fn new(ptr: *mut T) -> Self {
  102|    228|        Self { ptr }
  103|    228|    }
  104|       |
  105|      4|    pub fn as_ptr(&self) -> *mut T {
  106|      4|        self.ptr
  107|      4|    }
  108|       |}
  109|       |
  110|       |impl<T> Clone for SendPtr<T> {
  111|      0|    fn clone(&self) -> Self {
  112|      0|        *self
  113|      0|    }
  114|       |}
  115|       |
  116|       |impl<T> Copy for SendPtr<T> {}
  117|       |
  118|       |/// Runtime type information for garbage-collected objects.
  119|       |#[derive(Debug)]
  120|       |pub struct TypeInfo {
  121|       |    pub size: usize,
  122|       |    pub align: usize,
  123|       |    pub trace_fn: unsafe fn(*const (), &mut Vec<SendPtr<GcHeader<()>>>),
  124|       |    pub drop_fn: unsafe fn(*mut GcHeader<()>),
  125|       |    pub finalize_fn: Option<unsafe fn(*mut GcHeader<()>)>,
  126|       |    pub redirect_pointers_fn: unsafe fn(*mut GcHeader<()>, *mut GcHeader<()>, *mut GcHeader<()>),
  127|       |    pub object_type: ObjectType,
  128|       |}
  129|       |
  130|       |/// Header for all garbage-collected objects.
  131|       |#[derive(Debug)]
  132|       |#[repr(C)]
  133|       |pub struct GcHeader<T> {
  134|       |    pub mark_bit: AtomicBool,
  135|       |    pub type_info: &'static TypeInfo,
  136|       |    pub forwarding_ptr: AtomicPtr<GcHeader<()>>,
  137|       |    pub weak_ref_list: AtomicPtr<Weak<()>>,
  138|       |    pub data: T,
  139|       |}
  140|       |
  141|       |/// Smart pointer for garbage-collected objects.
  142|       |///
  143|       |/// `Gc<T>` is the primary interface for creating and accessing garbage-collected objects.
  144|       |/// It provides atomic pointer semantics with automatic forwarding pointer following
  145|       |/// and integration with the garbage collector's marking and sweeping phases.
  146|       |///
  147|       |/// # Examples
  148|       |///
  149|       |/// ```
  150|       |/// use fugrip::Gc;
  151|       |///
  152|       |/// // Create a garbage-collected integer
  153|       |/// let gc_int = Gc::new(42);
  154|       |/// 
  155|       |/// // Read the value
  156|       |/// let value = gc_int.read().unwrap();
  157|       |/// assert_eq!(*value, 42);
  158|       |/// ```
  159|       |///
  160|       |/// ```
  161|       |/// use fugrip::Gc;
  162|       |///
  163|       |/// // Create a garbage-collected string
  164|       |/// let gc_string = Gc::new("Hello, World!".to_string());
  165|       |/// 
  166|       |/// // Read and modify the value
  167|       |/// let value = gc_string.read().unwrap();
  168|       |/// assert_eq!(*value, "Hello, World!");
  169|       |/// ```
  170|       |/// Smart pointer for garbage-collected objects.
  171|       |///
  172|       |/// `Gc<T>` is the primary handle returned when allocating into the GC heap.
  173|       |/// It provides safe reader/writer guards via `read()` and `write()`.
  174|       |///
  175|       |/// # Examples
  176|       |///
  177|       |/// ```
  178|       |/// use fugrip::Gc;
  179|       |///
  180|       |/// // Allocate a simple integer in the GC heap and read it back.
  181|       |/// let g = Gc::new(42);
  182|       |/// if let Some(r) = g.read() {
  183|       |///     assert_eq!(*r, 42);
  184|       |/// }
  185|       |/// ```
  186|       |pub struct Gc<T> {
  187|       |    pub(crate) ptr: AtomicPtr<GcHeader<T>>,
  188|       |    pub(crate) _phantom: PhantomData<T>,
  189|       |}
  190|       |
  191|       |unsafe impl<T: Send> Send for Gc<T> {}
  192|       |unsafe impl<T: Sync> Sync for Gc<T> {}
  193|       |
  194|       |impl<T> Clone for Gc<T> {
  195|  1.00k|    fn clone(&self) -> Self {
  196|  1.00k|        Self {
  197|  1.00k|            ptr: AtomicPtr::new(self.ptr.load(std::sync::atomic::Ordering::Acquire)),
  198|  1.00k|            _phantom: PhantomData,
  199|  1.00k|        }
  200|  1.00k|    }
  201|       |}
  202|       |
  203|       |impl<T> Gc<T> {
  204|     85|    pub fn as_ptr(&self) -> *mut GcHeader<T> {
  205|     85|        self.ptr.load(std::sync::atomic::Ordering::Acquire)
  206|     85|    }
  207|       |}
  208|       |
  209|       |/// Weak reference to a garbage-collected object.
  210|       |///
  211|       |/// `Weak<T>` provides a non-owning reference to a garbage-collected object that
  212|       |/// does not prevent the object from being collected. Weak references are organized
  213|       |/// in doubly-linked lists and are automatically invalidated when their target
  214|       |/// object is garbage collected.
  215|       |///
  216|       |/// # Examples
  217|       |///
  218|       |/// ```
  219|       |/// use fugrip::{Gc, Weak};
  220|       |///
  221|       |/// // Create a strong reference
  222|       |/// let strong_ref = Gc::new(42i32);
  223|       |/// 
  224|       |/// // Create a weak reference to it
  225|       |/// let weak_ref = Weak::new_simple(&strong_ref);
  226|       |///
  227|       |/// // Weak references can be upgraded to strong references
  228|       |/// if let Some(upgraded) = weak_ref.read().unwrap().upgrade() {
  229|       |///     let value = upgraded.read().unwrap();
  230|       |///     assert_eq!(*value, 42);
  231|       |/// }
  232|       |/// ```
  233|       |/// Weak reference to a garbage-collected object.
  234|       |///
  235|       |/// A `Weak<T>` does not keep its target alive. Use `upgrade()` to attempt
  236|       |/// to turn a weak reference into a strong `Gc<T>`.
  237|       |///
  238|       |/// # Examples
  239|       |///
  240|       |/// ```
  241|       |/// use fugrip::{Gc, Weak};
  242|       |///
  243|       |/// let strong = Gc::new(7);
  244|       |/// let weak = Weak::new_simple(&strong);
  245|       |/// let maybe_upgraded = weak.read().unwrap().upgrade();
  246|       |/// assert!(maybe_upgraded.is_some());
  247|       |/// ```
  248|       |pub struct Weak<T> {
  249|       |    pub target: AtomicPtr<GcHeader<T>>,
  250|       |    pub next_weak: AtomicPtr<Weak<T>>,
  251|       |    pub prev_weak: AtomicPtr<Weak<T>>,
  252|       |}
  253|       |
  254|       |/// A reference to a weak reference that can be upgraded to a strong reference.
  255|       |pub struct WeakRef<T> {
  256|       |    pub weak: *const Weak<T>,
  257|       |    pub _phantom: PhantomData<T>,
  258|       |}
  259|       |
  260|       |/// Strong reference guard for reading GC objects.
  261|       |///
  262|       |/// `GcRef<'a, T>` provides safe, immutable access to garbage-collected objects
  263|       |/// for the duration of lifetime `'a`. It dereferences to `&T`, allowing
  264|       |/// transparent access to the underlying data.
  265|       |///
  266|       |/// # Examples
  267|       |///
  268|       |/// ```
  269|       |/// use fugrip::{Gc, GcRef};
  270|       |///
  271|       |/// let gc_value = Gc::new(String::from("Hello"));
  272|       |/// let guard = gc_value.read().unwrap();
  273|       |/// 
  274|       |/// // Access the value through the guard
  275|       |/// assert_eq!(guard.len(), 5);
  276|       |/// assert!(guard.starts_with("Hello"));
  277|       |/// ```
  278|       |/// Strong reference guard for reading GC objects.
  279|       |///
  280|       |/// `GcRef<'a, T>` dereferences to `&T` and is returned by `Gc::read()`.
  281|       |///
  282|       |/// # Examples
  283|       |///
  284|       |/// ```
  285|       |/// use fugrip::Gc;
  286|       |///
  287|       |/// let g = Gc::new(3i32);
  288|       |/// if let Some(r) = g.read() {
  289|       |///     assert_eq!(*r, 3i32);
  290|       |/// }
  291|       |/// ```
  292|       |pub struct GcRef<'a, T> {
  293|       |    ptr: *const T,
  294|       |    _phantom: PhantomData<&'a T>,
  295|       |}
  296|       |
  297|       |impl<'a, T> GcRef<'a, T> {
  298|       |    /// Creates a new GcRef from a raw pointer.
  299|       |    ///
  300|       |    /// # Safety
  301|       |    ///
  302|       |    /// The caller must ensure that:
  303|       |    /// - `ptr` points to a valid, initialized object of type `T`
  304|       |    /// - The object will remain valid for the lifetime `'a`
  305|       |    /// - The object is not being mutated while this reference exists
  306|       |    /// - The pointer is properly aligned and non-null
  307|  1.14k|    pub unsafe fn new(ptr: *const T) -> Self {
  308|  1.14k|        Self {
  309|  1.14k|            ptr,
  310|  1.14k|            _phantom: PhantomData,
  311|  1.14k|        }
  312|  1.14k|    }
  313|       |}
  314|       |
  315|       |impl<'a, T> std::ops::Deref for GcRef<'a, T> {
  316|       |    type Target = T;
  317|       |
  318|  1.13k|    fn deref(&self) -> &Self::Target {
  319|  1.13k|        unsafe { &*self.ptr }
  320|  1.13k|    }
  321|       |}
  322|       |
  323|       |/// Strong reference guard for writing GC objects.
  324|       |///
  325|       |/// `GcRefMut<'a, T>` provides safe, mutable access to garbage-collected objects
  326|       |/// for the duration of lifetime `'a`. It dereferences to `&mut T`, allowing
  327|       |/// transparent mutable access to the underlying data.
  328|       |///
  329|       |/// # Examples
  330|       |///
  331|       |/// ```
  332|       |/// use fugrip::Gc;
  333|       |///
  334|       |/// let gc_value = Gc::new(String::from("Hello"));
  335|       |/// if let Some(mut guard) = gc_value.write() {
  336|       |///     // Modify the value through the guard
  337|       |///     guard.push_str(", World!");
  338|       |///     assert_eq!(*guard, "Hello, World!");
  339|       |/// }
  340|       |/// 
  341|       |/// // Verify the change persisted
  342|       |/// let read_guard = gc_value.read().unwrap();
  343|       |/// assert_eq!(*read_guard, "Hello, World!");
  344|       |/// ```
  345|       |/// Strong reference guard for writing GC objects.
  346|       |///
  347|       |/// `GcRefMut<'a, T>` dereferences to `&mut T` and is returned by `Gc::write()`.
  348|       |///
  349|       |/// # Examples
  350|       |///
  351|       |/// ```
  352|       |/// use fugrip::Gc;
  353|       |///
  354|       |/// let g = Gc::new(1i32);
  355|       |/// if let Some(mut w) = g.write() {
  356|       |///     *w = 5;
  357|       |/// }
  358|       |/// if let Some(r) = g.read() {
  359|       |///     assert_eq!(*r, 5i32);
  360|       |/// }
  361|       |/// ```
  362|       |pub struct GcRefMut<'a, T> {
  363|       |    ptr: *mut T,
  364|       |    _phantom: PhantomData<&'a mut T>,
  365|       |}
  366|       |
  367|       |impl<'a, T> GcRefMut<'a, T> {
  368|       |    /// Creates a new GcRefMut from a raw pointer.
  369|       |    ///
  370|       |    /// # Safety
  371|       |    ///
  372|       |    /// The caller must ensure that:
  373|       |    /// - `ptr` points to a valid, initialized object of type `T`
  374|       |    /// - The object will remain valid for the lifetime `'a`
  375|       |    /// - No other references (mutable or immutable) to this object exist
  376|       |    /// - The pointer is properly aligned and non-null
  377|       |    /// - The caller has exclusive access to the object
  378|  1.00k|    pub unsafe fn new(ptr: *mut T) -> Self {
  379|  1.00k|        Self {
  380|  1.00k|            ptr,
  381|  1.00k|            _phantom: PhantomData,
  382|  1.00k|        }
  383|  1.00k|    }
  384|       |}
  385|       |
  386|       |impl<'a, T> std::ops::Deref for GcRefMut<'a, T> {
  387|       |    type Target = T;
  388|       |
  389|      0|    fn deref(&self) -> &Self::Target {
  390|      0|        unsafe { &*self.ptr }
  391|      0|    }
  392|       |}
  393|       |
  394|       |impl<'a, T> std::ops::DerefMut for GcRefMut<'a, T> {
  395|  1.00k|    fn deref_mut(&mut self) -> &mut Self::Target {
  396|  1.00k|        unsafe { &mut *self.ptr }
  397|  1.00k|    }
  398|       |}
  399|       |
  400|       |/// Singleton representing freed objects.
  401|       |pub struct FreeSingleton {
  402|       |    pub header: GcHeader<()>,
  403|       |}
  404|       |
  405|       |impl FreeSingleton {
  406|  2.36k|    pub fn instance() -> *mut GcHeader<()> {
  407|       |        static FREE_SINGLETON: std::sync::atomic::AtomicPtr<GcHeader<()>> =
  408|       |            std::sync::atomic::AtomicPtr::new(std::ptr::null_mut());
  409|       |
  410|  2.36k|        let ptr = FREE_SINGLETON.load(std::sync::atomic::Ordering::Acquire);
  411|  2.36k|        if ptr.is_null() {
  412|       |            // Initialize singleton
  413|      7|            let singleton = Box::leak(Box::new(FreeSingleton {
  414|      7|                header: GcHeader {
  415|      7|                    mark_bit: AtomicBool::new(true), // Always marked
  416|      7|                    type_info: &FREE_SINGLETON_TYPE_INFO,
  417|      7|                    forwarding_ptr: AtomicPtr::new(std::ptr::null_mut()),
  418|      7|                    weak_ref_list: AtomicPtr::new(std::ptr::null_mut()),
  419|      7|                    data: (),
  420|      7|                },
  421|      7|            }));
  422|      7|            let header_ptr = &mut singleton.header as *mut GcHeader<()>;
  423|       |
  424|      7|            match FREE_SINGLETON.compare_exchange(
  425|      7|                std::ptr::null_mut(),
  426|      7|                header_ptr,
  427|      7|                std::sync::atomic::Ordering::Release,
  428|      7|                std::sync::atomic::Ordering::Acquire,
  429|      7|            ) {
  430|      6|                Ok(_) => header_ptr,
  431|      1|                Err(existing) => existing, // Another thread beat us
  432|       |            }
  433|       |        } else {
  434|  2.36k|            ptr
  435|       |        }
  436|  2.36k|    }
  437|       |}
  438|       |
  439|       |pub const FREE_SINGLETON_TYPE_INFO: TypeInfo = TypeInfo {
  440|      0|    trace_fn: |_, _| {},
  441|      0|    drop_fn: |_: *mut GcHeader<()>| {},
  442|       |    finalize_fn: None,
  443|      0|    redirect_pointers_fn: |_, _, _| {},
  444|       |    size: std::mem::size_of::<GcHeader<()>>(),
  445|       |    align: std::mem::align_of::<GcHeader<()>>(),
  446|       |    object_type: ObjectType::Regular,
  447|       |};
  448|       |
  449|       |/// Returns type information for a given `GcTrace` type.
  450|       |///
  451|       |/// This function provides cached access to `TypeInfo` for garbage-collected types.
  452|       |/// It uses `TypeId`-based lookup to ensure each type gets a single, reusable
  453|       |/// `TypeInfo` instance with appropriate function pointers for tracing and dropping.
  454|       |///
  455|       |/// # Examples
  456|       |///
  457|       |/// ```
  458|       |/// use fugrip::{types::type_info, ObjectType};
  459|       |/// # use fugrip::traits::GcTrace;
  460|       |/// # use fugrip::SendPtr;
  461|       |/// # use fugrip::GcHeader;
  462|       |/// # struct TestType1(i32);
  463|       |/// # struct TestType2(String);
  464|       |/// # unsafe impl GcTrace for TestType1 { 
  465|       |/// #     unsafe fn trace(&self, _: &mut Vec<SendPtr<GcHeader<()>>>) {} 
  466|       |/// # }
  467|       |/// # unsafe impl GcTrace for TestType2 { 
  468|       |/// #     unsafe fn trace(&self, _: &mut Vec<SendPtr<GcHeader<()>>>) {} 
  469|       |/// # }
  470|       |///
  471|       |/// // Get type information for a test type
  472|       |/// let type1_info = type_info::<TestType1>();
  473|       |/// assert!(type1_info.size > 0);
  474|       |/// assert_eq!(type1_info.object_type, ObjectType::Regular);
  475|       |///
  476|       |/// // Type info is cached - same pointer returned for same type
  477|       |/// let type1_info2 = type_info::<TestType1>();
  478|       |/// assert!(std::ptr::eq(type1_info, type1_info2));
  479|       |///
  480|       |/// // Different types have different type info
  481|       |/// let type2_info = type_info::<TestType2>();
  482|       |/// assert!(!std::ptr::eq(type1_info, type2_info));
  483|       |/// ```
  484|       |///
  485|       |/// This version uses proper caching with TypeId-based lookup.
  486|  52.5k|pub fn type_info<T: crate::traits::GcTrace + 'static>() -> &'static TypeInfo {
  487|       |    use std::any::TypeId;
  488|       |    use std::collections::HashMap;
  489|       |    use std::sync::{LazyLock, Mutex};
  490|       |
  491|       |    static TYPE_INFO_CACHE: LazyLock<Mutex<HashMap<TypeId, &'static TypeInfo>>> =
  492|      6|        LazyLock::new(|| Mutex::new(HashMap::new()));
  493|       |
  494|  52.5k|    let type_id = TypeId::of::<T>();
  495|  52.5k|    let mut cache = TYPE_INFO_CACHE.lock().unwrap();
  496|       |
  497|  52.5k|    if let Some(&cached) = cache.get(&type_id) {
                               ^52.5k
  498|  52.5k|        return cached;
  499|     12|    }
  500|       |
  501|     12|    let type_info = Box::leak(Box::new(TypeInfo {
  502|     12|        size: std::mem::size_of::<GcHeader<T>>(),
  503|     12|        align: std::mem::align_of::<GcHeader<T>>(),
  504|       |        trace_fn: |obj, stack| unsafe {
  505|      0|            let header = &*(obj as *mut GcHeader<T>);
  506|      0|            header.data.trace(stack);
  507|      0|        },
  508|       |        drop_fn: |obj| unsafe {
  509|      0|            let header = obj as *mut GcHeader<T>;
  510|      0|            std::ptr::drop_in_place(&mut (*header).data);
  511|      0|        },
  512|     12|        finalize_fn: None,
  513|      0|        redirect_pointers_fn: |_, _, _| {},
  514|     12|        object_type: ObjectType::Regular,
  515|       |    }));
  516|       |
  517|     12|    cache.insert(type_id, type_info);
  518|     12|    type_info
  519|  52.5k|}
  520|       |
  521|       |/// Aligns a memory address up to the specified alignment boundary.
  522|       |///
  523|       |/// This function takes a memory address and returns the smallest address
  524|       |/// that is greater than or equal to the input address and is aligned to
  525|       |/// the specified boundary. This is commonly used in memory allocators
  526|       |/// to ensure proper alignment for different data types.
  527|       |///
  528|       |/// # Parameters
  529|       |///
  530|       |/// * `addr` - The input memory address to align
  531|       |/// * `align` - The alignment boundary (must be a power of 2)
  532|       |///
  533|       |/// # Examples
  534|       |///
  535|       |/// Basic alignment:
  536|       |///
  537|       |/// ```
  538|       |/// use fugrip::align_up;
  539|       |///
  540|       |/// // Align to 8-byte boundary
  541|       |/// let ptr = 0x1003 as *const u8;
  542|       |/// let aligned = align_up(ptr, 8);
  543|       |/// assert_eq!(aligned as usize, 0x1008);
  544|       |///
  545|       |/// // Already aligned addresses remain unchanged
  546|       |/// let ptr = 0x1000 as *const u8;
  547|       |/// let aligned = align_up(ptr, 8);
  548|       |/// assert_eq!(aligned as usize, 0x1000);
  549|       |/// ```
  550|       |///
  551|       |/// Different alignment boundaries:
  552|       |///
  553|       |/// ```
  554|       |/// use fugrip::align_up;
  555|       |///
  556|       |/// let addr = 0x1001 as *const u8;
  557|       |///
  558|       |/// // 4-byte alignment
  559|       |/// let aligned_4 = align_up(addr, 4);
  560|       |/// assert_eq!(aligned_4 as usize, 0x1004);
  561|       |///
  562|       |/// // 16-byte alignment  
  563|       |/// let aligned_16 = align_up(addr, 16);
  564|       |/// assert_eq!(aligned_16 as usize, 0x1010);
  565|       |///
  566|       |/// // 32-byte alignment
  567|       |/// let aligned_32 = align_up(addr, 32);
  568|       |/// assert_eq!(aligned_32 as usize, 0x1020);
  569|       |/// ```
  570|       |///
  571|       |/// Edge cases:
  572|       |///
  573|       |/// ```
  574|       |/// use fugrip::align_up;
  575|       |///
  576|       |/// // Null pointer remains null
  577|       |/// let null_ptr = std::ptr::null::<u8>();
  578|       |/// let aligned = align_up(null_ptr, 8);
  579|       |/// assert_eq!(aligned as usize, 0);
  580|       |///
  581|       |/// // Single byte alignment (no-op)
  582|       |/// let ptr = 0x1234 as *const u8;
  583|       |/// let aligned = align_up(ptr, 1);
  584|       |/// assert_eq!(aligned as usize, 0x1234);
  585|       |/// ```
  586|  52.5k|pub fn align_up(addr: *const u8, align: usize) -> *const u8 {
  587|  52.5k|    let addr = addr as usize;
  588|  52.5k|    let remainder = addr % align;
  589|  52.5k|    if remainder == 0 {
  590|  52.5k|        addr as *const u8
  591|       |    } else {
  592|      1|        (addr + align - remainder) as *const u8
  593|       |    }
  594|  52.5k|}
  595|       |
  596|       |/// Concrete allocation state for thread-local allocators.
  597|       |/// 
  598|       |/// This struct represents the actual state of a thread-local allocation buffer,
  599|       |/// while the `AllocationBuffer` trait in interfaces.rs defines the interface.
  600|       |pub struct ThreadLocalBuffer {
  601|       |    pub buffer: *mut u8,
  602|       |    pub position: *mut u8,
  603|       |    pub limit: *mut u8,
  604|       |    pub allocating_black: bool,
  605|       |}
  606|       |
  607|       |/// Statistics for garbage collection operations.
  608|       |#[derive(Debug, Default)]
  609|       |pub struct GcStats {
  610|       |    pub total_allocations: u64,
  611|       |    pub total_collections: u64,
  612|       |    pub bytes_allocated: u64,
  613|       |    pub bytes_freed: u64,
  614|       |    pub collection_time_ms: u64,
  615|       |}
  616|       |
  617|       |/// Configuration for garbage collector behavior.
  618|       |#[derive(Debug)]
  619|       |pub struct GcConfig {
  620|       |    pub initial_heap_size: usize,
  621|       |    pub max_heap_size: usize,
  622|       |    pub allocation_threshold: usize,
  623|       |    pub concurrent_marking: bool,
  624|       |    pub parallel_workers: usize,
  625|       |}
  626|       |
  627|       |impl Default for GcConfig {
  628|      3|    fn default() -> Self {
  629|      3|        Self {
  630|      3|            initial_heap_size: 1024 * 1024,    // 1MB
  631|      3|            max_heap_size: 1024 * 1024 * 1024, // 1GB
  632|      3|            allocation_threshold: 1024 * 1024, // 1MB
  633|      3|            concurrent_marking: true,
  634|      3|            parallel_workers: num_cpus::get(),
  635|      3|        }
  636|      3|    }
  637|       |}

